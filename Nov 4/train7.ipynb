{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTTOw7cYsO62",
        "outputId": "4a2da992-be8b-456f-b182-238df8d45e5e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Zv2N2Teaevhv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "from clip import clip\n",
        "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tn0hdtpuJ88B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed=0):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "seed_everything()"
      ],
      "metadata": {
        "id": "qwah0PjLr7EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_tokenizer = _Tokenizer()"
      ],
      "metadata": {
        "id": "ltbV0qIqvHOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Attention**"
      ],
      "metadata": {
        "id": "RTn1olOs2bOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContextAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads, dropout_rate = 0.3):\n",
        "        super(ContextAttention, self).__init__()\n",
        "        self.attention_layer = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first= True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, image_features, ctx_features):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_features: Tensor of shape [b, 196, 512] (batch size, seq length, feature size)\n",
        "            ctx_features: Tensor of shape [b, n_ctx, 512] (context features for 'real' and 'fake')\n",
        "        Returns:\n",
        "            Tensor of shape [b, n_ctx, 512] containing attended context features.\n",
        "        \"\"\"\n",
        "        attn_output, attn_weight = self.attention_layer(ctx_features, image_features, image_features)\n",
        "        attn_output = self.dropout(attn_output)\n",
        "        return attn_output, attn_weight # attn_output shape is (b, n_ctx, 512)"
      ],
      "metadata": {
        "id": "mRRho2pIfJMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PromptLearner**"
      ],
      "metadata": {
        "id": "kWLM5Hx12YkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PromptLearner(nn.Module):\n",
        "    def __init__(self, n_ctx, classnames, clip_model, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        dtype = torch.float32\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        ctx_dim = clip_model.ln_final.weight.shape[0] # clip_model.ln_final.weight.shape[0] is 512\n",
        "        self.n_cls = len(classnames)\n",
        "\n",
        "        ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=dtype, device = device)\n",
        "        nn.init.normal_(ctx_vectors, std=0.02)\n",
        "        self.ctx = nn.Parameter(ctx_vectors)  # to be optimized --> shape = [n_ctx, 512]\n",
        "\n",
        "        self.ctx_attn = ContextAttention(512, 8)\n",
        "\n",
        "        prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
        "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
        "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(device) # tokenized_prompts.shape is (2, 77)\n",
        "        with torch.no_grad():\n",
        "                embedding = clip_model.token_embedding(tokenized_prompts).type(dtype) # embedding.shape = torch.Size([2, 77, 512])\n",
        "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS --> shape = [2, 1, 512]\n",
        "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS -->shape = [2, 61, 512]\n",
        "        self.tokenized_prompts = tokenized_prompts # tokenized_prompts.shape is (2, 77)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "    def forward(self, image_features):\n",
        "        image_features = F.normalize(image_features, dim=1)  # shape = [b, 196, 512]\n",
        "        ctx_features = F.normalize(self.ctx, dim=1)  # shape = [n_ctx, 512]\n",
        "        # image_features = image_features / image_features.norm(dim=1, keepdim=True) # shape = [b, 196, 512]\n",
        "        # ctx_features = self.ctx / self.ctx.norm(dim=1, keepdim=True) # shape = [n_ctx, 512]\n",
        "        ctx_features = ctx_features[None, :, :] # shape = [1, n_ctx, 512]\n",
        "        ctx_features = ctx_features.repeat(image_features.shape[0], 1, 1) # shape = [b, n_ctx, 512]\n",
        "        ctx_features_attn_output, ctx_features_attn_weight = self.ctx_attn(image_features, ctx_features) # shape = [b, n_ctx, 512]\n",
        "        ctx_features_attn_output = ctx_features_attn_output + ctx_features # shape = [b, n_ctx, 512]\n",
        "        ctx_features_attn_output = ctx_features_attn_output.mean(dim = 0) # shape = [n_ctx, 512]\n",
        "        ctx_features_attn_output = ctx_features_attn_output.unsqueeze(0) # shape = [1, n_ctx, 512]\n",
        "        ctx_features_attn_output = ctx_features_attn_output.expand(self.n_cls, -1, -1) #shape = [2, n_ctx, 512]\n",
        "        ctx_features_attn_output = self.dropout(ctx_features_attn_output)\n",
        "\n",
        "\n",
        "        prompts = torch.cat([self.token_prefix, ctx_features_attn_output, self.token_suffix], dim=1) # shape = [2, 1 + n_ctx + * , 512]\n",
        "        # shape of prompts = [2, 77, 512]\n",
        "        return prompts\n"
      ],
      "metadata": {
        "id": "dqoHhGAadITe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TextEncoder**"
      ],
      "metadata": {
        "id": "U3KGMBIPwE-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This TextEncoder class is designed to process text using elements from the CLIP model's architecture.\n",
        "It uses a transformer module to encode a series of tokenized text prompts and extract features from the \"end of text\" (EOT) token,\n",
        "which often represents a summary of the input sequence in CLIP's text encoding.\n",
        "'''\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, clip_model, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        self.transformer = clip_model.transformer\n",
        "        self.positional_embedding = clip_model.positional_embedding\n",
        "        self.ln_final = clip_model.ln_final #layer normalization module\n",
        "        self.text_projection = clip_model.text_projection # A linear projection matrix to map transformer outputs to a feature space used by the CLIP model.\n",
        "        self.dtype = torch.float32\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, prompts, tokenized_prompts):\n",
        "        x = prompts + self.positional_embedding.type(self.dtype) # shape of positional_embedding = torch.Size([77, 512])\n",
        "        '''\n",
        "        x.shape = [ batch_size (N), n_ctx (L), transformer.width (D) ]\n",
        "        '''\n",
        "        x = F.normalize(x, dim=-1)  # Normalize along the feature dimension\n",
        "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
        "        x = self.ln_final(x).type(self.dtype)\n",
        "        x = self.dropout(x)\n",
        "        x = F.normalize(x, dim=-1)  # Normalize before projection\n",
        "\n",
        "        # x.shape is  torch.Size([2, 77, 512])\n",
        "\n",
        "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
        "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
        "        '''\n",
        "        x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] selects the feature vector corresponding to the EOT token for each prompt in the batch.\n",
        "        This vector is then linearly projected with self.text_projection, giving the final encoded feature vector for each text prompt.\n",
        "        '''\n",
        "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
        "        # self.text_projection.shape =  torch.Size([512, 512])\n",
        "        # x.shape =  torch.Size([2, 512])\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "tQBpNZglbnh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MultiscaleAdapter**"
      ],
      "metadata": {
        "id": "TYv8IBlN3s26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiscaleAdapter(nn.Module):\n",
        "    def __init__(self, input_dim, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(input_dim, input_dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(input_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(input_dim, input_dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(input_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(input_dim, input_dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(input_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Modify branches to avoid errors\n",
        "        self.branch_f = nn.Sequential(\n",
        "            nn.Conv2d(input_dim, input_dim, kernel_size=1, padding=0),\n",
        "            nn.BatchNorm2d(input_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(input_dim, input_dim, kernel_size=1, padding=0),\n",
        "            nn.BatchNorm2d(input_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.branch_g = nn.Sequential(\n",
        "            nn.Conv2d(input_dim, input_dim, kernel_size=1, padding=0),\n",
        "            nn.BatchNorm2d(input_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(input_dim, input_dim, kernel_size=3, padding=1, dilation=1),\n",
        "            nn.BatchNorm2d(input_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.branch_h = nn.Sequential(\n",
        "            nn.Conv2d(input_dim, input_dim, kernel_size=1, padding=0),\n",
        "            nn.BatchNorm2d(input_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(input_dim, input_dim, kernel_size=3, padding=1, dilation=1),\n",
        "            nn.BatchNorm2d(input_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Combine features and project back to original dimension\n",
        "        self.combine = nn.Sequential(\n",
        "            nn.Conv2d(input_dim * 3, input_dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(input_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.final_projection = nn.Sequential(\n",
        "            nn.Conv2d(input_dim, input_dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(input_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "       # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "      # Kaiming initialization for convolutional layers with ReLU activation\n",
        "      for m in self.modules():\n",
        "          if isinstance(m, nn.Conv2d):\n",
        "              nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "              if m.bias is not None:\n",
        "                  nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape = (32, 768, 196)\n",
        "        x = x.unsqueeze(2)\n",
        "        # Now x.shape =  torch.Size([32, 768, 1, 196])\n",
        "        x = x.reshape(x.shape[0], x.shape[1], int(math.sqrt(x.shape[3])), int(math.sqrt(x.shape[3])))\n",
        "        # x.shape =  torch.Size([32, 768, 14, 14])\n",
        "        x_fg_input = self.conv1(x)\n",
        "        x_h_input = self.conv2(x)\n",
        "        # Branch processing\n",
        "        x_f = F.normalize(self.branch_f(x_fg_input), dim=1)  # Normalize after branch processing\n",
        "        x_g = F.normalize(self.branch_g(x_fg_input), dim=1)\n",
        "        x_h = F.normalize(self.branch_h(x_h_input), dim=1)\n",
        "\n",
        "        # Concatenating along channel dimension and projecting back\n",
        "        x_out = torch.cat([x_f, x_g, x_h], dim=1)\n",
        "        x_out = self.combine(x_out)\n",
        "\n",
        "        # Adding the processed multi-scale features to the original input\n",
        "        x_out = self.final_projection(x_out) + F.normalize(self.conv3(x), dim=1)\n",
        "        x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], -1)\n",
        "        x_out = self.dropout(x_out)\n",
        "\n",
        "        return x_out"
      ],
      "metadata": {
        "id": "otexsyJWkyN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CustomCLIP**"
      ],
      "metadata": {
        "id": "30jOsBcMvrRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCLIP(nn.Module):\n",
        "    def __init__(self, n_ctx, classnames, clip_model, dropout_rate = 0.3):\n",
        "        super().__init__()\n",
        "        self.prompt_learner = PromptLearner(n_ctx, classnames, clip_model)\n",
        "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts # tokenized_prompts.shape is (2, 77)\n",
        "\n",
        "        self.image_encoder = clip_model.visual\n",
        "        self.patch_embedding = self.image_encoder.conv1\n",
        "        self.class_embedding = self.image_encoder.class_embedding\n",
        "        self.positional_embedding = self.image_encoder.positional_embedding\n",
        "        self.ln_pre = self.image_encoder.ln_pre\n",
        "        for param in self.image_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.text_encoder = TextEncoder(clip_model)\n",
        "        for param in self.text_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        embed_dim = self.image_encoder.ln_post.weight.shape[0]\n",
        "        self.multiscale_adapters = nn.ModuleList(\n",
        "                                    [MultiscaleAdapter(input_dim=embed_dim)\n",
        "                                    for _ in range(len(self.image_encoder.transformer.resblocks))]\n",
        "                                  )\n",
        "        self.image_features_proj = nn.Linear(embed_dim, 512)\n",
        "        self._initialize_weights()\n",
        "        self.dtype = torch.float32\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.temperature_scale = nn.Parameter(torch.tensor(10.0))\n",
        "        self.logit_scale = clip_model.logit_scale\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        # Custom initialization for image_features_proj\n",
        "        nn.init.xavier_uniform_(self.image_features_proj.weight)\n",
        "        nn.init.constant_(self.image_features_proj.bias, 0)\n",
        "\n",
        "\n",
        "    def forward(self, images):\n",
        "        x = self.patch_embedding(images.type(torch.float32))  # (32, 768, 14, 14)\n",
        "        x = x.reshape(x.shape[0], x.shape[1], -1).permute(0, 2, 1)  # (batch, num_patches, embed_dim) i.e; (32, 196, 768)\n",
        "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)\n",
        "        # Now x.shape is [batch_size, num_patches + 1, 768] i.e; torch.Size([32, 197, 768])\n",
        "        x = x + self.positional_embedding.to(x.dtype)\n",
        "        x = self.ln_pre(x) # shape = [32, 197, 768]\n",
        "        # Now x.shape is torch.Size([32, 197, 768])\n",
        "        #x = x[:,1:,:] # shape = [32, 196, 768]\n",
        "\n",
        "        intermediate_outputs = []\n",
        "        for i, block in enumerate(self.image_encoder.transformer.resblocks):\n",
        "            x = block(x) # shape = [32, 197, 768]\n",
        "            x_cls = x[:,0,:] # shape = [32, 768]\n",
        "            x_cls = x_cls.unsqueeze(1) # shape = [32, 1, 768]\n",
        "            intermediate_outputs.append(x)\n",
        "            x = x[:,1:,:] # shape = [32, 196, 768]\n",
        "            x = x.permute(0, 2, 1) # shape = [32, 768, 196]\n",
        "            x = self.multiscale_adapters[i](x) + x # output of MSA 's shape = [32, 768, 196]\n",
        "            x = x.permute(0, 2, 1) # shape = [32, 196, 768]\n",
        "            x = torch.cat([x_cls, x], dim=1) # shape = [32, 197, 768]\n",
        "            x = F.normalize(x, p=2, dim=-1)\n",
        "        image_features = self.image_features_proj(x) # shape = [32, 197, 512]\n",
        "        image_features_cls = image_features[:,0,:] # shape = [32, 512]\n",
        "        image_features_cls = image_features_cls.unsqueeze(1) # shape = [32, 1, 512]\n",
        "        image_features = image_features[:,1:,:] # shape = [32, 196, 512]\n",
        "        image_features = self.dropout(image_features)\n",
        "        with torch.no_grad():\n",
        "          promptlearner_outputs = self.prompt_learner(image_features) # shape = [2, 77, 512]\n",
        "          text_features = self.text_encoder(promptlearner_outputs, self.tokenized_prompts) # shape = [2, 512]\n",
        "        text_features = text_features.unsqueeze(0) # shape = [1, 2, 512]\n",
        "\n",
        "        # Expand text_features to match image_features' batch size\n",
        "        text_features_expanded = text_features.expand(image_features.size(0), -1, -1)  # Shape: (32, 2, 512)\n",
        "        #image_features_expanded = image_features_cls.unsqueeze(2)  # Shape: (32, 1, 1, 512)\n",
        "        text_features_expanded = text_features_expanded.unsqueeze(1)  # Shape: (32, 1, 2, 512)\n",
        "\n",
        "        #image_features_expanded = F.normalize(image_features_expanded, p=2, dim=-1)\n",
        "        text_features_expanded = F.normalize(text_features_expanded, p=2, dim=-1)\n",
        "        # cosine_similarities = F.cosine_similarity(image_features_expanded, text_features_expanded, dim=-1)\n",
        "        # Cosine similarities shape: torch.Size([32, 1, 2])\n",
        "\n",
        "        image_features_cls = F.normalize(image_features_cls, p=2, dim=-1)\n",
        "\n",
        "\n",
        "        # cosine_similarities = cosine_similarities.squeeze(1) # Shape: (32, 2)\n",
        "        logits = torch.matmul(image_features_cls, text_features_expanded.squeeze(1).transpose(1, 2))\n",
        "        logits = logits.squeeze(1) # Shape: (32, 2)\n",
        "\n",
        "\n",
        "        #return cosine_similarities * self.temperature_scale\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        return logits * logit_scale\n"
      ],
      "metadata": {
        "id": "EIsmbLZYvu2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "knVsEgarCz-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/Zero_Shot_DeepFake_Image_Classification/CLIP_based_deepfake_detection/dataset/CELEB'\n",
        "\n",
        "# Define image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]),\n",
        "])\n",
        "\n",
        "# Load the dataset\n",
        "train_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'train'), transform=transform)\n",
        "train_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "val_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'val'), transform=transform)\n",
        "val_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers= os.cpu_count())\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers= os.cpu_count())"
      ],
      "metadata": {
        "id": "mathQ86cC1in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
        "    # Lambda function for warmup phase\n",
        "    def lr_lambda(current_step: int):\n",
        "      if current_step < num_warmup_steps:\n",
        "          return float(current_step) / float(max(1, num_warmup_steps))\n",
        "      return max(\n",
        "          0.0, 0.5 * (1.0 + torch.cos(torch.tensor(torch.pi * (current_step - num_warmup_steps) / (num_training_steps - num_warmup_steps))))\n",
        "      )\n",
        "    return LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, device, num_epochs=10, lr=1e-4, warmup_steps=100):\n",
        "    model.train()\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    #optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay= 1e-5)\n",
        "    optimizer = optim.SGD(\n",
        "                    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                    lr=lr,\n",
        "                    momentum=0.9,\n",
        "                    weight_decay = 1e-4\n",
        "                    )\n",
        "    total_steps = len(train_loader) * num_epochs\n",
        "    scheduler = get_scheduler(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    #criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            #labels_one_hot = F.one_hot(labels, num_classes=2).float()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():\n",
        "                logits = model(images)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            # Apply gradient clipping before optimizer step\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            scheduler.step()  # Update learning rate after every optimizer step\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = correct / total\n",
        "\n",
        "        val_loss, val_acc = validate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
        "              f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_acc:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "def validate_model(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            #labels_one_hot = F.one_hot(labels, num_classes=2).float()\n",
        "\n",
        "            with autocast():  # Use autocast in validation as well, if mixed precision\n",
        "                logits = model(images)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_loss = running_loss / total\n",
        "    val_acc = correct / total\n",
        "    return val_loss, val_acc"
      ],
      "metadata": {
        "id": "WNVqMhQ7DTiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# small_train_loader = torch.utils.data.DataLoader(\n",
        "#     torch.utils.data.Subset(train_dataset, range(32)),\n",
        "#     batch_size=32, shuffle=True\n",
        "# )\n",
        "# small_val_loader = torch.utils.data.DataLoader(\n",
        "#     torch.utils.data.Subset(val_dataset, range(32)),\n",
        "#     batch_size=32, shuffle=False\n",
        "# )"
      ],
      "metadata": {
        "id": "RzL4xWEmv4g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "clip_model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
        "clip_model = clip_model.float()\n",
        "\n",
        "n_ctx = 15  # Number of context tokens\n",
        "classnames = [\"real\", \"fake\"]\n",
        "\n",
        "model = CustomCLIP(n_ctx=n_ctx, classnames=classnames, clip_model=clip_model).to(device)\n",
        "train_model(model, train_loader, val_loader, device, num_epochs=20, lr=1e-1, warmup_steps=500)\n",
        "#train_model(model, small_train_loader, small_val_loader, device, num_epochs=5, lr=1e-2, warmup_steps=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "HiMI_PTwDXJs",
        "outputId": "eda34e9a-27ca-4bd5-b3c7-cf689f74388f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-f0653e6d9a6c>:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "Training Epoch 1/20:   0%|          | 0/500 [00:00<?, ?it/s]<ipython-input-11-f0653e6d9a6c>:39: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "Training Epoch 1/20: 100%|██████████| 500/500 [03:10<00:00,  2.62it/s]\n",
            "<ipython-input-11-f0653e6d9a6c>:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():  # Use autocast in validation as well, if mixed precision\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Train Loss: 1.5025, Train Accuracy: 0.5038, Val Loss: 0.7054, Val Accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/20: 100%|██████████| 500/500 [01:46<00:00,  4.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/20], Train Loss: 0.6956, Train Accuracy: 0.4994, Val Loss: 0.6917, Val Accuracy: 0.5160\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/20: 100%|██████████| 500/500 [01:45<00:00,  4.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/20], Train Loss: 0.6934, Train Accuracy: 0.4969, Val Loss: 0.6930, Val Accuracy: 0.5240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/20: 100%|██████████| 500/500 [01:46<00:00,  4.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/20], Train Loss: 0.6934, Train Accuracy: 0.4998, Val Loss: 0.6938, Val Accuracy: 0.4360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5/20: 100%|██████████| 500/500 [01:45<00:00,  4.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/20], Train Loss: 0.6934, Train Accuracy: 0.5001, Val Loss: 0.6930, Val Accuracy: 0.5960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6/20: 100%|██████████| 500/500 [01:45<00:00,  4.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/20], Train Loss: 0.6935, Train Accuracy: 0.4976, Val Loss: 0.6941, Val Accuracy: 0.3960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7/20: 100%|██████████| 500/500 [01:45<00:00,  4.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/20], Train Loss: 0.6934, Train Accuracy: 0.4936, Val Loss: 0.6936, Val Accuracy: 0.4600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8/20: 100%|██████████| 500/500 [01:45<00:00,  4.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/20], Train Loss: 0.6934, Train Accuracy: 0.5011, Val Loss: 0.6931, Val Accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9/20: 100%|██████████| 500/500 [01:45<00:00,  4.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/20], Train Loss: 0.6934, Train Accuracy: 0.5000, Val Loss: 0.6933, Val Accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10/20: 100%|██████████| 500/500 [01:45<00:00,  4.75it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d2907e283199>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomCLIP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_ctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclip_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#train_model(model, small_train_loader, small_val_loader, device, num_epochs=5, lr=1e-2, warmup_steps=10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-f0653e6d9a6c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, device, num_epochs, lr, warmup_steps)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mepoch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
            "\u001b[0;32m<ipython-input-11-f0653e6d9a6c>\u001b[0m in \u001b[0;36mvalidate_model\u001b[0;34m(model, val_loader, criterion, device)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Use autocast in validation as well, if mixed precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-ebec47a305e4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mintermediate_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# shape = [32, 197, 768]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mx_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# shape = [32, 768]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mx_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# shape = [32, 1, 768]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/clip/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/clip/model.py\u001b[0m in \u001b[0;36mattention\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1933\u001b[0m         )\n\u001b[1;32m   1934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1935\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Module\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1936\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mremove_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdicts_or_sets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdicts_or_sets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing**"
      ],
      "metadata": {
        "id": "MoftDFmxD2-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the testing function\n",
        "def test_model(model, test_loader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    criterion = nn.CrossEntropyLoss()  # Loss function for testing\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient tracking\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            with autocast():  # Use autocast in validation as well, if mixed precision\n",
        "                logits = model(images)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    test_loss = running_loss / total\n",
        "    test_acc = correct / total\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "QDI_fs0DED8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CELEB-Test**"
      ],
      "metadata": {
        "id": "JJ6bA_8MI_rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'test'), transform=transform)\n",
        "test_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers= os.cpu_count())"
      ],
      "metadata": {
        "id": "yvFF-RbCD4lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model after training\n",
        "test_model(model, test_loader, device)"
      ],
      "metadata": {
        "id": "0wQHUwC-EGtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CELEB-M**"
      ],
      "metadata": {
        "id": "HaeP1i46YE1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = '/content/drive/MyDrive/Zero_Shot_DeepFake_Image_Classification/CLIP_based_deepfake_detection/dataset/CELEB-M'\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(dir, 'test'), transform=transform)\n",
        "test_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers= os.cpu_count())"
      ],
      "metadata": {
        "id": "eMWVB_MaYE1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model after training\n",
        "test_model(model, test_loader, device)"
      ],
      "metadata": {
        "id": "WZaXuZCTYE1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FS**"
      ],
      "metadata": {
        "id": "B-LOBIofYLAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = '/content/drive/MyDrive/Zero_Shot_DeepFake_Image_Classification/CLIP_based_deepfake_detection/dataset/FS'\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(dir, 'test'), transform=transform)\n",
        "test_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers= os.cpu_count())"
      ],
      "metadata": {
        "id": "ywTod3gaYLAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model after training\n",
        "test_model(model, test_loader, device)"
      ],
      "metadata": {
        "id": "GuseiNQCYLAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NT**"
      ],
      "metadata": {
        "id": "C4OI2qNcJ4H7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = '/content/drive/MyDrive/Zero_Shot_DeepFake_Image_Classification/CLIP_based_deepfake_detection/dataset/NT'\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(dir, 'test'), transform=transform)\n",
        "test_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers= os.cpu_count())"
      ],
      "metadata": {
        "id": "ez2YL8U9J4H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model after training\n",
        "test_model(model, test_loader, device)"
      ],
      "metadata": {
        "id": "_zp1t3YsJ4H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DF**"
      ],
      "metadata": {
        "id": "2YAWPQs4KPtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = '/content/drive/MyDrive/Zero_Shot_DeepFake_Image_Classification/CLIP_based_deepfake_detection/dataset/DF'\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(dir, 'test'), transform=transform)\n",
        "test_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers= os.cpu_count())"
      ],
      "metadata": {
        "id": "1nbWt6nIKPtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model after training\n",
        "test_model(model, test_loader, device)"
      ],
      "metadata": {
        "id": "S62Djg4cKPtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DFD**"
      ],
      "metadata": {
        "id": "MHEX3FiHLUw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = '/content/drive/MyDrive/Zero_Shot_DeepFake_Image_Classification/CLIP_based_deepfake_detection/dataset/DFD'\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(dir, 'test'), transform=transform)\n",
        "test_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers= os.cpu_count())"
      ],
      "metadata": {
        "id": "UEaT19PTLUw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model after training\n",
        "test_model(model, test_loader, device)"
      ],
      "metadata": {
        "id": "Ymn2NaBkLUw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **F2F**"
      ],
      "metadata": {
        "id": "-UDHXxUbLbV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = '/content/drive/MyDrive/Zero_Shot_DeepFake_Image_Classification/CLIP_based_deepfake_detection/dataset/F2F'\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(dir, 'test'), transform=transform)\n",
        "test_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers= os.cpu_count())"
      ],
      "metadata": {
        "id": "cJN0h9azLbV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model after training\n",
        "test_model(model, test_loader, device)"
      ],
      "metadata": {
        "id": "Ylv15-IPLbV4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}