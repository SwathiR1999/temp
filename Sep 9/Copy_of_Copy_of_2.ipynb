{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "from transformers import ViTModel, ViTFeatureExtractor, ASTFeatureExtractor\n",
        "from transformers import ASTConfig, ASTModel"
      ],
      "metadata": {
        "id": "UEfa3oYhahVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "def resize_spectrogram(spectrogram, target_size=1024):\n",
        "    spectrogram = spectrogram.T\n",
        "    current_size = spectrogram.shape[0]  # Get the first dimension (height)\n",
        "\n",
        "    if current_size < target_size:\n",
        "        padding = target_size - current_size\n",
        "        padded_spectrogram = np.pad(spectrogram, ((0, padding), (0, 0)), mode='constant')\n",
        "        return padded_spectrogram\n",
        "    else:\n",
        "        truncated_spectrogram = spectrogram[:target_size, :]\n",
        "        return truncated_spectrogram"
      ],
      "metadata": {
        "id": "N9AAoWFEyn3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dataset class\n",
        "class ImageAudioDataset(Dataset):\n",
        "    def __init__(self, images_folder, spectrograms_folder, image_transform=None, spectrogram_transform=None):\n",
        "        self.images_folder = images_folder\n",
        "        self.spectrograms_folder = spectrograms_folder\n",
        "        self.image_transform = image_transform\n",
        "        self.spectrogram_transform = spectrogram_transform\n",
        "        self.image_filenames = sorted([f for f in os.listdir(images_folder) if f.endswith('.jpg')])\n",
        "        self.spectrogram_filenames = sorted([f for f in os.listdir(spectrograms_folder) if f.endswith('.jpg')])\n",
        "        assert len(self.image_filenames) == len(self.spectrogram_filenames), \"Mismatch between image and spectrogram files\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_filename = self.image_filenames[idx]\n",
        "        spectrogram_filename = self.spectrogram_filenames[idx]\n",
        "\n",
        "        image_path = os.path.join(self.images_folder, image_filename)\n",
        "        spectrogram_path = os.path.join(self.spectrograms_folder, spectrogram_filename)\n",
        "\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        #spectrogram = Image.open(spectrogram_path).convert('L')\n",
        "        spectrogram = cv2.imread(spectrogram_path, cv2.IMREAD_GRAYSCALE)\n",
        "        spectrogram = resize_spectrogram(spectrogram)\n",
        "\n",
        "\n",
        "        #print(spectrogram.shape)\n",
        "\n",
        "        if self.image_transform:\n",
        "            image = self.image_transform(image)\n",
        "        if self.spectrogram_transform:\n",
        "            spectrogram = self.spectrogram_transform(spectrogram)\n",
        "\n",
        "        return image, spectrogram\n",
        "\n",
        "# Define the models\n",
        "class ImageFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "    def forward(self,x_image):\n",
        "        image_outputs = self.vit(x_image)\n",
        "        return image_outputs.last_hidden_state.mean(dim=1)  # Mean pooling\n",
        "\n",
        "class AudioFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.ast = ASTModel.from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\n",
        "        # # Initializing a AST MIT/ast-finetuned-audioset-10-10-0.4593 style configuration\n",
        "        # configuration = ASTConfig(max_length = 128)\n",
        "\n",
        "        # # Initializing a model (with random weights) from the MIT/ast-finetuned-audioset-10-10-0.4593 style configuration\n",
        "        # self.ast = ASTModel(configuration).from_pretrained('MIT/ast-finetuned-audioset-10-10-0.4593')\n",
        "\n",
        "    def forward(self,x_audio):\n",
        "        audio_outputs = self.ast(x_audio.squeeze(dim=1))\n",
        "        return audio_outputs.last_hidden_state.mean(dim=1)  # Mean pooling\n",
        "\n",
        "class FocalAttention(nn.Module):\n",
        "    def __init__(self, dim_image, dim_audio):\n",
        "        super().__init__()\n",
        "        self.dim_image = dim_image\n",
        "        self.dim_audio = dim_audio\n",
        "        self.fc_image = nn.Linear(dim_image, dim_audio)\n",
        "        self.fc_audio = nn.Linear(dim_audio, dim_image)\n",
        "\n",
        "    def forward(self, image_features, audio_features):\n",
        "        image_features = self.fc_image(image_features)\n",
        "        audio_features = self.fc_audio(audio_features)\n",
        "        return image_features, audio_features"
      ],
      "metadata": {
        "id": "4GgIy65jaryJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageAudioMatchingModel(nn.Module):\n",
        "    def __init__(self, image_feature_extractor, audio_feature_extractor, focal_attention):\n",
        "        super().__init__()\n",
        "        self.image_feature_extractor = image_feature_extractor\n",
        "        self.audio_feature_extractor = audio_feature_extractor\n",
        "        self.focal_attention = focal_attention\n",
        "\n",
        "    def forward(self, image, audio):\n",
        "        image_features = self.image_feature_extractor(image)\n",
        "        audio_features = self.audio_feature_extractor(audio)\n",
        "        image_embeddings, audio_embeddings = self.focal_attention(image_features, audio_features)\n",
        "        return image_embeddings, audio_embeddings"
      ],
      "metadata": {
        "id": "T69RkK3zavi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.2):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, image_embeddings, audio_embeddings):\n",
        "        # Calculate cosine similarity\n",
        "        cos_sim = nn.functional.cosine_similarity(image_embeddings.unsqueeze(1), audio_embeddings.unsqueeze(0), dim=2)\n",
        "        positive_pair_sim = torch.diagonal(cos_sim)\n",
        "        hardest_negative_image = cos_sim.max(dim=1)[0]\n",
        "        hardest_negative_audio = cos_sim.max(dim=0)[0]\n",
        "\n",
        "        loss = torch.mean(\n",
        "            torch.clamp(self.alpha - positive_pair_sim + hardest_negative_image, min=0) +\n",
        "            torch.clamp(self.alpha - positive_pair_sim + hardest_negative_audio, min=0)\n",
        "        )\n",
        "        return loss"
      ],
      "metadata": {
        "id": "Gqgbf418ayRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define image and audio transformations\n",
        "image_transform = T.Compose([\n",
        "    T.Resize((224, 224)),  # Resize to the size expected by ViT and AST\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "spectrogram_transform = T.Compose([\n",
        "    #T.Resize((512,128)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.5], std=[0.5]),\n",
        "])\n",
        "\n",
        "#spectrogram_transform = T.ToTensor()"
      ],
      "metadata": {
        "id": "Ui_UkuiWa05j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize dataset and dataloader\n",
        "images_folder = 'images'\n",
        "spectrograms_folder = 'spectrograms'\n",
        "dataset = ImageAudioDataset(images_folder=images_folder, spectrograms_folder=spectrograms_folder, image_transform=image_transform, spectrogram_transform=spectrogram_transform)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "P6onvzK-a4aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models, loss function, and optimizer\n",
        "image_feature_extractor = ImageFeatureExtractor()\n",
        "audio_feature_extractor = AudioFeatureExtractor()\n",
        "focal_attention = FocalAttention(dim_image=768, dim_audio=768)  # Adjust dimensions as needed\n",
        "model = ImageAudioMatchingModel(image_feature_extractor, audio_feature_extractor, focal_attention)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = ContrastiveLoss(alpha=0.2)"
      ],
      "metadata": {
        "id": "4in11vsQa7zV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "456c5c03-0fe8-4dea-977d-1083f96ce4fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train_model(num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for images, spectrograms in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            image_embeddings, audio_embeddings = model(images, spectrograms)\n",
        "            loss = criterion(image_embeddings, audio_embeddings)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(dataloader)}')"
      ],
      "metadata": {
        "id": "N9hEWFqLa_jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "V2F0ZTNIbCEP",
        "outputId": "21acd18d-1c71-4d82-fae0-43938e949960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.4362578123807907\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-077dd4aaaf40>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-1084733904c6>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(num_epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mimage_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspectrograms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define test dataset class (same as training dataset class but for test data)\n",
        "class TestImageAudioDataset(Dataset):\n",
        "    def __init__(self, images_folder, spectrograms_folder, image_transform=None, spectrogram_transform=None):\n",
        "        self.images_folder = images_folder\n",
        "        self.spectrograms_folder = spectrograms_folder\n",
        "        self.image_transform = image_transform\n",
        "        self.spectrogram_transform = spectrogram_transform\n",
        "        self.image_filenames = sorted([f for f in os.listdir(images_folder) if f.endswith('.jpg')])\n",
        "        self.spectrogram_filenames = sorted([f for f in os.listdir(spectrograms_folder) if f.endswith('.jpg')])\n",
        "        assert len(self.image_filenames) == len(self.spectrogram_filenames), \"Mismatch between image and spectrogram files\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_filename = self.image_filenames[idx]\n",
        "        spectrogram_filename = self.spectrogram_filenames[idx]\n",
        "\n",
        "        image_path = os.path.join(self.images_folder, image_filename)\n",
        "        spectrogram_path = os.path.join(self.spectrograms_folder, spectrogram_filename)\n",
        "\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        spectrogram = cv2.imread(spectrogram_path, cv2.IMREAD_GRAYSCALE)\n",
        "        spectrogram = resize_spectrogram(spectrogram)\n",
        "\n",
        "        if self.image_transform:\n",
        "            image = self.image_transform(image)\n",
        "        if self.spectrogram_transform:\n",
        "            spectrogram = self.spectrogram_transform(spectrogram)\n",
        "\n",
        "        return image, spectrogram\n",
        "\n",
        "# Initialize test dataset and dataloader\n",
        "test_images_folder = 'images'\n",
        "test_spectrograms_folder = 'spectrograms'\n",
        "test_dataset = TestImageAudioDataset(images_folder=test_images_folder, spectrograms_folder=test_spectrograms_folder, image_transform=image_transform, spectrogram_transform=spectrogram_transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "l6wF0zbBjqdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_recall_at_1(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_image_features = []\n",
        "    all_audio_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, spectrograms in dataloader:\n",
        "            images = images.to(device)\n",
        "            spectrograms = spectrograms.to(device)\n",
        "            image_embeddings, audio_embeddings = model(images, spectrograms)\n",
        "\n",
        "            all_image_features.append(image_embeddings.cpu())\n",
        "            all_audio_features.append(audio_embeddings.cpu())\n",
        "            all_labels.append(torch.arange(len(images)).cpu())\n",
        "\n",
        "    # Concatenate all features and labels\n",
        "    all_image_features = torch.cat(all_image_features, dim=0)\n",
        "    all_audio_features = torch.cat(all_audio_features, dim=0)\n",
        "    all_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    cos_sim = nn.functional.cosine_similarity(all_image_features.unsqueeze(1), all_audio_features.unsqueeze(0), dim=2)\n",
        "\n",
        "    # Calculate Recall@1\n",
        "    recall_at_1 = 0\n",
        "    num_samples = len(all_image_features)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        sorted_indices = torch.argsort(cos_sim[i], descending=True)\n",
        "        if sorted_indices[0] == i:\n",
        "            recall_at_1 += 1\n",
        "\n",
        "    recall_at_1 = recall_at_1 / num_samples\n",
        "    return recall_at_1"
      ],
      "metadata": {
        "id": "NCzINWbGR8j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to appropriate device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Calculate Recall@1\n",
        "recall_at_1 = compute_recall_at_1(model, test_dataloader, device)\n",
        "print(f'Recall@1: {recall_at_1:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4RQ-LPHSEPd",
        "outputId": "af7ab5c9-e15f-462a-df7f-bb25d51e323d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall@1: 0.0500\n"
          ]
        }
      ]
    }
  ]
}