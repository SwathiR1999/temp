{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/Aerial_Scene_Recognition/ClassificationAfterFinetune/without_finetune'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TkjOCZYOuPV",
        "outputId": "6b400c74-5dcf-417f-f56c-bba2b501887a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Aerial_Scene_Recognition/ClassificationAfterFinetune/without_finetune\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 43\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "E_-10zkSRIFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xkX-7W3i551K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU7da441Lnee"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "# Load image embeddings\n",
        "with h5py.File('train_val_vision_embeddings.h5', 'r') as f:\n",
        "    image_embeddings = f['train_val_vision_embeddings'][:]\n",
        "\n",
        "# Load audio embeddings\n",
        "with h5py.File('train_val_audio_embeddings.h5', 'r') as f:\n",
        "    audio_embeddings = f['train_val_audio_embeddings'][:]\n",
        "\n",
        "# Load labels\n",
        "labels = np.load('train_val_labels_inputs.npy')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_embeddings = torch.tensor(image_embeddings, dtype=torch.float32)\n",
        "audio_embeddings = torch.tensor(audio_embeddings, dtype=torch.float32)\n",
        "labels = torch.tensor(labels, dtype=torch.long)"
      ],
      "metadata": {
        "id": "a2Qu5ymOUXmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = torch.utils.data.TensorDataset(image_embeddings, audio_embeddings, labels)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True) # original batch size = 32, but 16 is better"
      ],
      "metadata": {
        "id": "s3-lYuikUg_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import math\n",
        "\n",
        "# Dual Attention Mechanism\n",
        "class DualAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads=8):\n",
        "        super(DualAttention, self).__init__()\n",
        "        self.image_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        self.audio_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        self.cross_modal_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, image_embeddings, audio_embeddings):\n",
        "        # Self-attention on image embeddings\n",
        "        img_attn_output, _ = self.image_attn(image_embeddings, image_embeddings, image_embeddings)\n",
        "        img_attn_output = self.norm(img_attn_output + image_embeddings)\n",
        "\n",
        "        # Self-attention on audio embeddings\n",
        "        audio_attn_output, _ = self.audio_attn(audio_embeddings, audio_embeddings, audio_embeddings)\n",
        "        audio_attn_output = self.norm(audio_attn_output + audio_embeddings)\n",
        "\n",
        "        # Cross-attention between image and audio embeddings\n",
        "        combined_attn_output, _ = self.cross_modal_attn(img_attn_output, audio_attn_output, audio_attn_output)\n",
        "        combined_attn_output = self.norm(combined_attn_output + img_attn_output)\n",
        "\n",
        "        return combined_attn_output\n",
        "\n",
        "# Transformer Classifier\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, num_heads=8, num_layers=2, dim_feedforward=2048, dropout=0.4):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.dual_attention = DualAttention(input_dim, num_heads)\n",
        "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=num_layers)\n",
        "        self.norm1 = nn.LayerNorm(input_dim)\n",
        "        self.fc = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, image_embeddings, audio_embeddings):\n",
        "        # Apply dual attention\n",
        "        combined_embeddings = self.dual_attention(image_embeddings.unsqueeze(1), audio_embeddings.unsqueeze(1))\n",
        "\n",
        "        # Pass through transformer encoder\n",
        "        combined_embeddings = self.transformer_encoder(combined_embeddings)\n",
        "\n",
        "        # Apply normalization\n",
        "        combined_embeddings = self.norm1(combined_embeddings.mean(dim=1))\n",
        "\n",
        "        # # Final classification layer\n",
        "        x = self.fc(combined_embeddings)\n",
        "\n",
        "        return x\n",
        "\n",
        "input_dim = image_embeddings.shape[1]  # Assuming image_embeddings and audio_embeddings have the same dimension\n",
        "num_classes = 13  # Number of classes\n",
        "model = TransformerClassifier(input_dim=input_dim, num_classes=num_classes)"
      ],
      "metadata": {
        "id": "nXRYbe7lL17l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cca240c0-a023-4c76-b232-7be459815887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6, weight_decay=1e-5)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 15\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    batch_count = 0\n",
        "    for image_embeds, audio_embeds, targets in dataloader:\n",
        "        batch_count += 1\n",
        "        sys.stdout.write(f\"\\rBatch {batch_count}/{len(dataloader)}\")\n",
        "        sys.stdout.flush()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(image_embeds, audio_embeds)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss}\")\n"
      ],
      "metadata": {
        "id": "E3ZE7k9FL9ke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1729569f-36b0-43cb-b1db-b576174d7922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1003/1003Epoch 1, Average Loss: 2.2243900965836088\n",
            "Batch 1003/1003Epoch 2, Average Loss: 1.8224411077299718\n",
            "Batch 1003/1003Epoch 3, Average Loss: 1.502573927506017\n",
            "Batch 1003/1003Epoch 4, Average Loss: 1.2491040423973012\n",
            "Batch 1003/1003Epoch 5, Average Loss: 1.0472587918118015\n",
            "Batch 1003/1003Epoch 6, Average Loss: 0.9048308894030772\n",
            "Batch 1003/1003Epoch 7, Average Loss: 0.7882042894602297\n",
            "Batch 1003/1003Epoch 8, Average Loss: 0.7065881743046603\n",
            "Batch 1003/1003Epoch 9, Average Loss: 0.6383417873556985\n",
            "Batch 1003/1003Epoch 10, Average Loss: 0.5873920267126556\n",
            "Batch 1003/1003Epoch 11, Average Loss: 0.5417045954102325\n",
            "Batch 1003/1003Epoch 12, Average Loss: 0.5054458988605741\n",
            "Batch 1003/1003Epoch 13, Average Loss: 0.47533517484372095\n",
            "Batch 1003/1003Epoch 14, Average Loss: 0.446203023754246\n",
            "Batch 1003/1003Epoch 15, Average Loss: 0.425170752500274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TESTING..**"
      ],
      "metadata": {
        "id": "bNF028DqObZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test image embeddings\n",
        "with h5py.File('test_vision_embeddings.h5', 'r') as f:\n",
        "    test_image_embeddings = f['test_vision_embeddings'][:]\n",
        "\n",
        "# Load test audio embeddings\n",
        "with h5py.File('test_audio_embeddings.h5', 'r') as f:\n",
        "    test_audio_embeddings = f['test_audio_embeddings'][:]\n",
        "\n",
        "# Load test labels\n",
        "test_labels = np.load('test_labels_inputs.npy')\n",
        "\n",
        "# # Load test image embeddings\n",
        "# with h5py.File('test_vision_embeddings.h5', 'r') as f:\n",
        "#     test_image_embeddings = f['test_vision_embeddings'][:]\n",
        "\n",
        "# # Load test audio embeddings\n",
        "# with h5py.File('test_audio_embeddings.h5', 'r') as f:\n",
        "#     test_audio_embeddings = f['test_audio_embeddings'][:]\n",
        "\n",
        "# # Load test labels\n",
        "# test_labels = np.load('test_labels_inputs.npy')"
      ],
      "metadata": {
        "id": "S1fTNWulOeao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_image_embeddings = l2_normalize(test_image_embeddings)\n",
        "# test_audio_embeddings = l2_normalize(test_audio_embeddings)"
      ],
      "metadata": {
        "id": "FvHFDVe3zBM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "test_image_embeddings = torch.tensor(test_image_embeddings, dtype=torch.float32)\n",
        "test_audio_embeddings = torch.tensor(test_audio_embeddings, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test_labels, dtype=torch.long)"
      ],
      "metadata": {
        "id": "d9zHFStnW2_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = torch.utils.data.TensorDataset(test_image_embeddings, test_audio_embeddings, test_labels)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "4DSJEdnuXFYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize lists to store true labels and predictions\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# No gradient calculation needed during evaluation\n",
        "with torch.no_grad():\n",
        "    for image_embeds, audio_embeds, labels in test_dataloader:\n",
        "        # Forward pass\n",
        "        outputs = model(image_embeds, audio_embeds)\n",
        "\n",
        "        # Get predicted class\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        # Append predictions and true labels to lists\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "id": "MDdVoLtJXJCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed7aedb8-a0e8-4014-c7a3-5349f335ae15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9539\n",
            "Recall: 0.9530\n",
            "F1 Score: 0.9502\n"
          ]
        }
      ]
    }
  ]
}