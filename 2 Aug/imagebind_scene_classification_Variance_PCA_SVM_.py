# -*- coding: utf-8 -*-
"""Imagebind_scene_classification_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZjmeNFWzLxpgXn3qIkjKa2kdUxCo6Kjh
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Aerial_Scene_Recognition/

# ! pip install pytorch-lightning --quiet

from pytorch_lightning import seed_everything
seed_everything(43, workers=True)

import os
current_directory = os.getcwd()

import numpy as np
label_list = np.load('train_val_labels_inputs.npy').tolist()

import h5py

import numpy as np
file_path = 'train_val_vision_embeddings.h5'

# Open the HDF5 file
with h5py.File(file_path, 'r') as h5f:
    # Access the dataset
    dataset = h5f['train_val_vision_embeddings']

    # Load the data into a NumPy array
    vision_embeddings = np.array(dataset)

print("Embeddings loaded from 'train_val_vision_embeddings.h5'")
print(vision_embeddings.shape)  # Print the shape of the loaded embeddings

file_path = 'train_val_audio_embeddings.h5'

# Open the HDF5 file
with h5py.File(file_path, 'r') as h5f:
    # Access the dataset
    dataset = h5f['train_val_audio_embeddings']

    # Load the data into a NumPy array
    audio_embeddings = np.array(dataset)

print("Embeddings loaded from 'train_val_audio_embeddings.h5'")
print(audio_embeddings.shape)  # Print the shape of the loaded embeddings

X = []
y = []
for i in range(vision_embeddings.shape[0]):
  concatenated_embedding = np.array([vision_embeddings[i], audio_embeddings[i]])
  X.append(concatenated_embedding)
  y.append(label_list[i])

from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=43)

X_train, X_val, y_train, y_val = np.array(X_train), np.array(X_val), np.array(y_train), np.array(y_val)

vision_emb_train_only = []
audio_emb_train_only = []
for i in range(X_train.shape[0]):
  vision_emb_train_only.append(X_train[i][0])
  audio_emb_train_only.append(X_train[i][1])

vision_emb_val_only = []
audio_emb_val_only = []
for i in range(X_val.shape[0]):
  vision_emb_val_only.append(X_val[i][0])
  audio_emb_val_only.append(X_val[i][1])

vision_emb_train_only = np.array(vision_emb_train_only)
audio_emb_train_only = np.array(audio_emb_train_only)

vision_emb_val_only = np.array(vision_emb_val_only)
audio_emb_val_only = np.array(audio_emb_val_only)

from sklearn.feature_selection import VarianceThreshold
#####################################################################################################################
var_thr_vision = VarianceThreshold(threshold=0.00015)
var_thr_audio = VarianceThreshold(threshold=0.65)

vision_emb_train_only = var_thr_vision.fit_transform(vision_emb_train_only)
audio_emb_train_only = var_thr_audio.fit_transform(audio_emb_train_only)

vision_emb_val_only = var_thr_vision.transform(vision_emb_val_only)
audio_emb_val_only = var_thr_audio.transform(audio_emb_val_only)

X_train_temp = []
for i in range(X_train.shape[0]):
  X_train_temp.append(np.concatenate((vision_emb_train_only[i], audio_emb_train_only[i])))
X_train = np.array(X_train_temp)

X_val_temp = []
for i in range(X_val.shape[0]):
  X_val_temp.append(np.concatenate((vision_emb_val_only[i], audio_emb_val_only[i])))
X_val = np.array(X_val_temp)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)

print("After variance threshold, number of features = ",X_train.shape[1])

from sklearn.decomposition import PCA
#####################################################################################################################
pca = PCA(n_components=int(X_train.shape[1]*0.5))
X_train = pca.fit_transform(X_train)
X_val = pca.transform(X_val)

from sklearn.svm import SVC
svm_classifier = SVC(kernel='rbf')  #'rbf
svm_classifier.fit(X_train, y_train)

from sklearn.metrics import precision_recall_fscore_support, accuracy_score

y_pred = svm_classifier.predict(X_val)
accuracy = accuracy_score(y_val, y_pred)

precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average='weighted')

print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Precision: {precision * 100:.2f}%")
print(f"Recall: {recall * 100:.2f}%")
print(f"F1 Score: {f1 * 100:.2f}%")

"""# **Testing**"""

test_label_list = np.load('test_labels_inputs.npy').tolist()

file_path = 'test_vision_embeddings.h5'

# Open the HDF5 file
with h5py.File(file_path, 'r') as h5f:
    # Access the dataset
    dataset = h5f['test_vision_embeddings']

    # Load the data into a NumPy array
    vision_embeddings = np.array(dataset)

print("Embeddings loaded from 'test_vision_embeddings.h5'")
print(vision_embeddings.shape)

file_path = 'test_audio_embeddings.h5'

# Open the HDF5 file
with h5py.File(file_path, 'r') as h5f:
    # Access the dataset
    dataset = h5f['test_audio_embeddings']

    # Load the data into a NumPy array
    audio_embeddings = np.array(dataset)

print("Embeddings loaded from 'test_audio_embeddings.h5'")
print(audio_embeddings.shape)

vision_embeddings = var_thr_vision.transform(vision_embeddings)
audio_embeddings = var_thr_audio.transform(audio_embeddings)

X = []
y = []
for i in range(vision_embeddings.shape[0]):
  concatenated_embedding = np.concatenate((vision_embeddings[i], audio_embeddings[i]))
  X.append(concatenated_embedding)
  y.append(test_label_list[i])

X= scaler.transform(X)

X = pca.transform(X)

y_pred = svm_classifier.predict(X)

accuracy = accuracy_score(y, y_pred)

# Calculate precision, recall, and F1 score
precision, recall, f1, _ = precision_recall_fscore_support(y, y_pred, average='weighted')

print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Precision: {precision * 100:.2f}%")
print(f"Recall: {recall * 100:.2f}%")
print(f"F1 Score: {f1 * 100:.2f}%")