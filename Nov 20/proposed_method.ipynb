{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTTOw7cYsO62",
        "outputId": "a16e722c-4350-4e08-b61c-c055bf142d49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXxOnZnhPHF1"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wovIKsWiPHF2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from PIL import Image\n",
        "import io\n",
        "from collections import OrderedDict\n",
        "from typing import Tuple, Union\n",
        "\n",
        "from clip import clip\n",
        "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SE85gj-nPHF2"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed=0):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "seed_everything()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjO8Fw6EPHF2"
      },
      "outputs": [],
      "source": [
        "_tokenizer = _Tokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2pAHgeJdDHi"
      },
      "outputs": [],
      "source": [
        "# # batch_first=False --> the expected input format is (sequence_length, batch_size, embedding_dim)\n",
        "# # batch_first= True --> the expected input format is (batch_size, sequence_length, embedding_dim)\n",
        "\n",
        "# class ContextAttention(nn.Module):\n",
        "#     def __init__(self, embed_dim, image_dim, num_heads, dtype):\n",
        "#         super(ContextAttention, self).__init__()\n",
        "#         self.attention_layer = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True).to(dtype)\n",
        "#         self.img_projection = nn.Linear(image_dim, embed_dim).to(dtype)\n",
        "\n",
        "#     def forward(self, image_features, ctx_features):\n",
        "#         \"\"\"\n",
        "#         Args:\n",
        "#             image_features: Tensor of shape [b, 196, 768] (batch size, seq length, feature size)\n",
        "#             ctx_feature: Tensor of shape [b, n_ctx, 512]\n",
        "#         Returns:\n",
        "#             Tensor of shape [b, n_ctx, 512] containing attended context features.\n",
        "#         \"\"\"\n",
        "#         device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#         ctx_features = ctx_features.to(device)\n",
        "\n",
        "#         image_features = self.img_projection(image_features).to(device) # shape = (b, 196, 512)\n",
        "\n",
        "#         attn_output, attn_weight = self.attention_layer(ctx_features, image_features, image_features)\n",
        "\n",
        "#         return attn_output, attn_weight # attn_output shape is (b, n_ctx, 512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKiCQ6SxuAZ5"
      },
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, clip_model):\n",
        "        super().__init__()\n",
        "        self.transformer = clip_model.transformer\n",
        "        self.positional_embedding = clip_model.positional_embedding\n",
        "        self.ln_final = clip_model.ln_final\n",
        "        self.text_projection = clip_model.text_projection\n",
        "        self.dtype = clip_model.dtype\n",
        "\n",
        "    def forward(self, prompts, tokenized_prompts):\n",
        "\n",
        "        x = prompts + self.positional_embedding.type(self.dtype)\n",
        "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
        "        x = self.ln_final(x).type(self.dtype)\n",
        "\n",
        "\n",
        "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
        "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
        "        x = x[torch.arange(x.shape[0]), [tokenized_prompts.argmax(dim=-1)]*x.shape[0]] @ self.text_projection\n",
        "\n",
        "        # tokenized_prompts has shape [77]\n",
        "        # tokenized_prompts.argmax(dim=-1) will give list i.e; [19]\n",
        "\n",
        "\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bi__ADHGu30Y"
      },
      "outputs": [],
      "source": [
        "class PromptLearner(nn.Module):\n",
        "    def __init__(self, cfg, classnames, clip_model, image_dim):\n",
        "        super().__init__()\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        n_cls = len(classnames)\n",
        "        n_ctx = cfg.TRAINER.COOP.N_CTX\n",
        "        ctx_init = cfg.TRAINER.COOP.CTX_INIT\n",
        "        dtype = clip_model.dtype\n",
        "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
        "        clip_imsize = clip_model.visual.input_resolution\n",
        "        cfg_imsize = cfg.INPUT.SIZE[0]\n",
        "        assert cfg_imsize == clip_imsize, f\"cfg_imsize ({cfg_imsize}) must equal to clip_imsize ({clip_imsize})\"\n",
        "\n",
        "        if ctx_init:\n",
        "            # use given words to initialize context vectors\n",
        "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
        "            n_ctx = len(ctx_init.split(\" \"))\n",
        "            prompt = clip.tokenize(ctx_init).to(device)\n",
        "            with torch.no_grad():\n",
        "                embedding = clip_model.token_embedding(prompt).type(dtype)\n",
        "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
        "            prompt_prefix = ctx_init\n",
        "\n",
        "        else:\n",
        "            # random initialization\n",
        "            if cfg.TRAINER.COOP.CSC:\n",
        "                print(\"Initializing class-specific contexts\")\n",
        "                ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim, dtype=dtype, device = device)\n",
        "            else:\n",
        "                print(\"Initializing a generic context\")\n",
        "                ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=dtype, device = device)\n",
        "            nn.init.normal_(ctx_vectors, std=0.02)\n",
        "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
        "\n",
        "        print(f'Initial context: \"{prompt_prefix}\"')\n",
        "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
        "\n",
        "        self.ctx = nn.Parameter(ctx_vectors)  # to be optimized\n",
        "\n",
        "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
        "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
        "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
        "\n",
        "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(device)\n",
        "        with torch.no_grad():\n",
        "            embedding = clip_model.token_embedding(tokenized_prompts).type(dtype)\n",
        "\n",
        "        # These token vectors will be saved when in save_model(),\n",
        "        # but they should be ignored in load_model() as we want to use\n",
        "        # those computed using the current class names\n",
        "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
        "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS\n",
        "\n",
        "        self.n_cls = n_cls\n",
        "        self.n_ctx = n_ctx\n",
        "        self.tokenized_prompts = tokenized_prompts  # torch.Tensor\n",
        "        self.name_lens = name_lens\n",
        "        self.class_token_position = cfg.TRAINER.COOP.CLASS_TOKEN_POSITION\n",
        "\n",
        "        self.img_projection = nn.Linear(image_dim, ctx_dim).to(dtype)\n",
        "        self.ctx_attn = nn.MultiheadAttention(embed_dim=ctx_dim, num_heads=4, batch_first=True, dropout=0.3).to(dtype)\n",
        "        self.ctx_norm1 = nn.LayerNorm(ctx_dim).to(dtype)\n",
        "        self.ctx_norm2 = nn.LayerNorm(ctx_dim).to(dtype)\n",
        "\n",
        "\n",
        "    def forward(self, image_features):\n",
        "        ctx = self.ctx  # (n_cls, n_ctx, dim)\n",
        "        if ctx.dim() == 2:\n",
        "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
        "\n",
        "\n",
        "        ctx1 = ctx[0].unsqueeze(0).expand(image_features.shape[0], -1, -1) # (b, n_ctx, dim)\n",
        "        ctx2 = ctx[1].unsqueeze(0).expand(image_features.shape[0], -1, -1) # (b, n_ctx, dim)\n",
        "\n",
        "\n",
        "        projected_image_features = self.img_projection(image_features) # (b, 196, dim)\n",
        "        ctx_attn_output1, ctx_attn_weight1 = self.ctx_attn(ctx1, projected_image_features, projected_image_features)\n",
        "        ctx1 = ctx1 + ctx_attn_output1\n",
        "        ctx1 = self.ctx_norm1(ctx1)\n",
        "\n",
        "        ctx_attn_output2, ctx_attn_weight2 = self.ctx_attn(ctx2, projected_image_features, projected_image_features)\n",
        "        ctx2 = ctx2 + ctx_attn_output2\n",
        "        ctx2 = self.ctx_norm2(ctx2)\n",
        "\n",
        "\n",
        "        prefix1 = self.token_prefix[0] # (1, dim) ----> shape of self.token_prefix = (n_cls, 1, dim)\n",
        "        suffix1 = self.token_suffix[0] # (*, dim) ----> shape of self.token_prefix = (n_cls, *, dim)\n",
        "        prefix2 = self.token_prefix[1] # (1, dim)\n",
        "        suffix2 = self.token_suffix[1] # (*, dim)\n",
        "\n",
        "\n",
        "        prefix1 = prefix1.unsqueeze(0).expand(image_features.shape[0], -1, -1) # (b, 1, dim)\n",
        "        suffix1 = suffix1.unsqueeze(0).expand(image_features.shape[0], -1, -1) # (b, *, dim)\n",
        "        prefix2 = prefix2.unsqueeze(0).expand(image_features.shape[0], -1, -1) # (b, 1, dim)\n",
        "        suffix2 = suffix2.unsqueeze(0).expand(image_features.shape[0], -1, -1) # (b, *, dim)\n",
        "\n",
        "\n",
        "        prompts1 = torch.cat(\n",
        "                            [prefix1, # (b, 1, dim)\n",
        "                             ctx1,    # (b, n_ctx, dim)\n",
        "                             suffix1  # (b, *, dim)\n",
        "                            ], dim=1 )\n",
        "\n",
        "        prompts2 = torch.cat(\n",
        "                            [prefix2, # (b, 1, dim)\n",
        "                             ctx2,    # (b, n_ctx, dim)\n",
        "                             suffix2  # (b, *, dim)\n",
        "                            ], dim=1 )\n",
        "\n",
        "\n",
        "        return prompts1, prompts2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IB1j1DRx-8nf"
      },
      "outputs": [],
      "source": [
        "class MultiscaleAdapter(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch_f = nn.Sequential(\n",
        "            nn.Conv2d(input_dim, input_dim, kernel_size=1, padding=0, dilation=1),\n",
        "            #nn.BatchNorm2d(input_dim),\n",
        "            nn.LayerNorm([input_dim, 14, 14]),\n",
        "            #nn.ReLU()\n",
        "            nn.GELU()\n",
        "        )\n",
        "        self.branch_g = nn.Sequential(\n",
        "            nn.Conv2d(input_dim, input_dim, kernel_size=3, padding=3, dilation=3),\n",
        "            #nn.BatchNorm2d(input_dim),\n",
        "            nn.LayerNorm([input_dim, 14, 14]),\n",
        "            #nn.ReLU()\n",
        "            nn.GELU()\n",
        "        )\n",
        "        self.branch_h = nn.Sequential(\n",
        "            nn.Conv2d(input_dim, input_dim, kernel_size=5, padding=10, dilation=5),\n",
        "            #nn.BatchNorm2d(input_dim),\n",
        "            nn.LayerNorm([input_dim, 14, 14]),\n",
        "            #nn.ReLU()\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        # Combine features and project back to original dimension\n",
        "        self.combine = nn.Sequential(\n",
        "            nn.Conv2d(input_dim * 3, input_dim, kernel_size=1),\n",
        "            #nn.BatchNorm2d(input_dim),\n",
        "            nn.LayerNorm([input_dim, 14, 14]),\n",
        "            #nn.ReLU(),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(p=0.3)\n",
        "        )\n",
        "\n",
        "        self.weight_f = nn.Parameter(torch.ones(1))\n",
        "        self.weight_g = nn.Parameter(torch.ones(1))\n",
        "        self.weight_h = nn.Parameter(torch.ones(1))\n",
        "\n",
        "        self.se_block = nn.Sequential(\n",
        "                        nn.AdaptiveAvgPool2d(1),\n",
        "                        nn.Conv2d(input_dim, input_dim // 4, kernel_size=1),\n",
        "                        nn.LayerNorm([input_dim // 4, 1, 1]),\n",
        "                        nn.GELU(),\n",
        "                        nn.Conv2d(input_dim // 4, input_dim, kernel_size=1),\n",
        "                        nn.LayerNorm([input_dim, 1, 1]),\n",
        "                        nn.Sigmoid()\n",
        "                    )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape = (32, 768, 196)\n",
        "        x = x.unsqueeze(2)\n",
        "        # Now x.shape =  torch.Size([32, 768, 1, 196])\n",
        "        x = x.reshape(x.shape[0], x.shape[1], int(math.sqrt(x.shape[3])), int(math.sqrt(x.shape[3])))\n",
        "        # x.shape =  torch.Size([32, 768, 14, 14])\n",
        "        x_f = self.branch_f(x)\n",
        "        x_g = self.branch_g(x)\n",
        "        x_h = self.branch_h(x)\n",
        "        # Concatenate along channel dimension and project back\n",
        "        # x_out = torch.cat([x_f, x_g, x_h], dim=1)\n",
        "        x_out = torch.cat([self.weight_f * x_f, self.weight_g * x_g, self.weight_h * x_h], dim=1)\n",
        "        x_out = self.combine(x_out) # shape = [32, 768, 14, 14]\n",
        "        #x_out = x_out * self.se_block(x_out)\n",
        "        x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], -1) # shape = [32, 768, 196]\n",
        "        return x_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CGYwvLjPHF4"
      },
      "outputs": [],
      "source": [
        "class CustomCLIP(nn.Module):\n",
        "    def __init__(self, cfg, classnames, clip_model):\n",
        "        super().__init__()\n",
        "        self.image_encoder = clip_model.visual\n",
        "        image_dim = self.image_encoder.ln_post.weight.shape[0]\n",
        "        text_dim = clip_model.token_embedding.weight.shape[1]\n",
        "        self.dtype = clip_model.dtype\n",
        "        self.prompt_learner = PromptLearner(cfg, classnames, clip_model, image_dim)\n",
        "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
        "        self.patch_embedding = self.image_encoder.conv1\n",
        "        self.class_embedding = self.image_encoder.class_embedding\n",
        "        self.positional_embedding = self.image_encoder.positional_embedding\n",
        "        self.ln_pre = self.image_encoder.ln_pre\n",
        "        self.ln_post = self.image_encoder.ln_post\n",
        "        self.patch_norm1 = nn.LayerNorm(image_dim).to(self.dtype)\n",
        "        self.patch_norm2 = nn.LayerNorm(image_dim).to(self.dtype)\n",
        "\n",
        "        self.text_encoder = TextEncoder(clip_model)\n",
        "        self.multiscale_adapters = nn.ModuleList(\n",
        "                                    [MultiscaleAdapter(input_dim=image_dim).to(self.dtype)\n",
        "                                    for _ in range(len(self.image_encoder.transformer.resblocks))]\n",
        "                                  )\n",
        "        self.patch_attn = nn.MultiheadAttention(embed_dim=image_dim, num_heads=4, batch_first=True, dropout=0.3).to(self.dtype)\n",
        "        self.image_cls_proj = self.image_encoder.proj\n",
        "        self.logit_scale = clip_model.logit_scale\n",
        "\n",
        "    def forward(self, images):\n",
        "        x = self.patch_embedding(images.type(self.dtype))  # (32, 768, 14, 14)\n",
        "        x = x.reshape(x.shape[0], x.shape[1], -1).permute(0, 2, 1)  # (batch, num_patches, embed_dim) i.e; (32, 196, 768)\n",
        "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)\n",
        "        x = x + self.positional_embedding.to(x.dtype)\n",
        "        x = self.ln_pre(x)\n",
        "        num_layers = len(self.image_encoder.transformer.resblocks)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        for i, block in enumerate(self.image_encoder.transformer.resblocks):\n",
        "            x = x.permute(1, 0, 2) # shape = [197, 32, 768]\n",
        "            x = block(x) # shape = [197, 32, 768]\n",
        "            x = x.permute(1, 0, 2) # shape = [32, 197, 768]\n",
        "            # if i < (num_layers - 1):\n",
        "            # if i in [6, 7, 8, 9, 10]:\n",
        "            if i in [7, 8, 9, 10]:\n",
        "              x_cls = x[:, 0] # shape = [32, 768]\n",
        "              x_cls = x_cls.unsqueeze(1) # shape = [32, 1, 768]\n",
        "              x_patches = x[:, 1:] # shape = [32, 196, 768]\n",
        "              x_patches = x_patches.permute(0, 2, 1) # shape = [32, 768, 196]\n",
        "              x_patches_msa = self.multiscale_adapters[i](x_patches) # shape = [32, 768, 196]\n",
        "              x_patches_msa = x_patches_msa.permute(0, 2, 1) # shape = [32, 196, 768]\n",
        "              x_patches = x_patches.permute(0, 2, 1) # shape = [32, 196, 768]\n",
        "              x_patches = self.patch_attn(x_patches_msa, x_patches, x_patches)[0] + x_patches\n",
        "              x_patches = self.patch_norm1(x_patches)\n",
        "              x = torch.cat([x_cls, x_patches], dim=1) # shape = [32, 197, 768]\n",
        "\n",
        "\n",
        "        image_patches = x[:, 1:] # shape = [32, 196, 768]\n",
        "\n",
        "        promptlearner_output1, promptlearner_output2  = self.prompt_learner(image_patches)\n",
        "        # shape of each promptlearner_output --> [32, 77, 512]\n",
        "\n",
        "        text_features1 = self.text_encoder(promptlearner_output1, self.tokenized_prompts[0]) # shape = [32, 512]\n",
        "        text_features2 = self.text_encoder(promptlearner_output2, self.tokenized_prompts[1]) # shape = [32, 512]\n",
        "\n",
        "\n",
        "\n",
        "        image_cls = x[:, 0] # shape = [32, 768]\n",
        "        image_cls = self.ln_post(image_cls) # shape = [32, 768]\n",
        "        image_cls = image_cls @ self.image_cls_proj # shape = [32, 512]\n",
        "        image_cls = image_cls.unsqueeze(1) # shape = [32, 1, 512]\n",
        "\n",
        "\n",
        "        image_cls = image_cls / image_cls.norm(dim=-1, keepdim=True)\n",
        "        text_features1 = text_features1 / text_features1.norm(dim=-1, keepdim=True)\n",
        "        text_features2 = text_features2 / text_features2.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        text_features = torch.stack([text_features1, text_features2], dim=1) # shape = [32, 2, 512]\n",
        "\n",
        "        text_features = text_features.permute(0, 2, 1) # shape = [32, 512, 2]\n",
        "\n",
        "\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        logits = logit_scale * image_cls @ text_features\n",
        "        logits = logits.squeeze(1) # (32, 2)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaZEI7NjPHF4"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuzARAtoPHF5"
      },
      "outputs": [],
      "source": [
        "# Custom function to simulate compression artifacts\n",
        "def compress_blur(image):\n",
        "    # Save the image temporarily in JPEG format to simulate compression\n",
        "    buffer = io.BytesIO()  # Create an in-memory byte buffer\n",
        "    image.save(buffer, format=\"JPEG\", quality=75)  # Adjust quality as needed for stronger compression\n",
        "    buffer.seek(0)  # Move to the start of the buffer\n",
        "    return Image.open(buffer)\n",
        "\n",
        "# Define the transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224), interpolation=Image.BICUBIC),\n",
        "    transforms.RandomResizedCrop(224),  # Random crop within (224, 224)\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.Lambda(lambda img: compress_blur(img)),  # Apply compression blur simulation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224), interpolation= InterpolationMode.BICUBIC),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdcqRFADPHF6"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content/drive/MyDrive/Zero_Shot_DeepFake_Image_Classification/CLIP_based_deepfake_detection/dataset/F2F'\n",
        "\n",
        "# Load the dataset\n",
        "train_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'train'), transform=transform)\n",
        "train_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "val_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'val'), transform=test_transform)\n",
        "val_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers= os.cpu_count())\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers= os.cpu_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjGYp1OmPHF6"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, device, num_epochs=8, lr=0.001, print_freq=125): # 0.001\n",
        "    model.train()\n",
        "\n",
        "    optimizer = optim.SGD(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()),\n",
        "        lr=lr,\n",
        "        momentum=0.9,\n",
        "        weight_decay=1e-4\n",
        "    )\n",
        "\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0.00001)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        if (epoch+1) == 4:\n",
        "          break\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
        "\n",
        "\n",
        "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "        for batch_idx, (images, labels) in enumerate(tqdm(train_loader, desc=f\"Training\")):\n",
        "\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            logits = model(images)\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            # Gradient clipping\n",
        "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            if (batch_idx + 1) % print_freq == 0:\n",
        "                batch_loss = loss.item()\n",
        "                batch_acc = (predicted == labels).sum().item() / labels.size(0)\n",
        "                print(f\"Batch [{batch_idx + 1}/{len(train_loader)}], Loss: {batch_loss:.4f}, Accuracy: {batch_acc:.4f}\")\n",
        "\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = correct / total\n",
        "\n",
        "        val_loss, val_acc = validate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
        "              f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_acc:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "\n",
        "def validate_model(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_loss = running_loss / total\n",
        "    val_acc = correct / total\n",
        "    return val_loss, val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4fQOxcyPHF7"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        # General Trainer configuration\n",
        "        self.TRAINER = TrainerConfig()\n",
        "        # Input configuration (image size, etc.)\n",
        "        self.INPUT = InputConfig()\n",
        "\n",
        "class TrainerConfig:\n",
        "    def __init__(self):\n",
        "        # COOP-specific settings\n",
        "        self.COOP = CoopConfig()\n",
        "\n",
        "class CoopConfig:\n",
        "    def __init__(self):\n",
        "        # Number of context words in the prompt\n",
        "        self.N_CTX = 64  # This can be changed as needed #16\n",
        "        # Initial prompt context, set to None if random initialization is desired\n",
        "        #self.CTX_INIT = \"This photo is\"  # Initial context string\n",
        "        self.CTX_INIT = None\n",
        "        # Whether to use class-specific contexts\n",
        "        self.CSC = True  # Set True if you want class-specific contexts\n",
        "        # Position of the class token in the prompt (e.g., \"front\", \"middle\", \"end\")\n",
        "        self.CLASS_TOKEN_POSITION = \"end\"\n",
        "\n",
        "class InputConfig:\n",
        "    def __init__(self):\n",
        "        # Image size expected by the model (e.g., 224 for 224x224 images)\n",
        "        self.SIZE = [224]  # Ensure this matches the CLIP model input size\n",
        "\n",
        "# Instantiate the configuration\n",
        "cfg = Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c2_ER7TZ5AJ",
        "outputId": "8935cef1-66c0-42dd-c52e-fd6f443d6da1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing class-specific contexts\n",
            "Initial context: \"X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X\"\n",
            "Number of context words (tokens): 64\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "clip_model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
        "\n",
        "# clip_model = clip_model.float()\n",
        "\n",
        "classnames = [\"pristine\", \"forged\"]\n",
        "\n",
        "model = CustomCLIP(cfg = cfg, classnames=classnames, clip_model=clip_model).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Thebg1E3JZlC"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "  if all(keyword not in name for keyword in [ \"prompt_learner\", \"multiscale_adapters\", \"patch_norm1\", \"patch_norm2\", \"patch_attn\"]):\n",
        "    param.requires_grad_(False)\n",
        "# prompt_learner, multiscale_adapters, patch_norm, patch_attn --> requires grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "o2N7CeKeJ37c",
        "outputId": "7d0bb2a0-013b-43fd-e629-7ada2defb3dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8, Learning Rate: 0.000962320368593087\n",
            "\n",
            "Epoch 1/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   2%|▏         | 24/1000 [00:48<32:51,  2.02s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-180824f006da>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-43-1aeb7f39188f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, device, num_epochs, lr, print_freq)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nEpoch {epoch + 1}/{num_epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1413\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_model(model, train_loader, val_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoftDFmxD2-Z"
      },
      "source": [
        "# **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDI_fs0DED8a"
      },
      "outputs": [],
      "source": [
        "# Define the testing function\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def test_model(model, test_loader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    criterion = nn.CrossEntropyLoss()  # Loss function for testing\n",
        "\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient tracking\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Collect true labels and predictions for confusion matrix\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    test_loss = running_loss / total\n",
        "    test_acc = correct / total\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    # Generate and display the confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions, labels=[0, 1])\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Real', 'Fake'])\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ6bA_8MI_rC"
      },
      "source": [
        "# **CELEB-Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvFF-RbCD4lI"
      },
      "outputs": [],
      "source": [
        "dir = '/content/drive/MyDrive/Zero_Shot_DeepFake_Image_Classification/CLIP_based_deepfake_detection/dataset/CELEB'\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(dir, 'test'), transform=test_transform)\n",
        "test_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers= os.cpu_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wQHUwC-EGtK"
      },
      "outputs": [],
      "source": [
        "# Test the model after training\n",
        "test_model(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-LOBIofYLAq"
      },
      "source": [
        "# **FS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywTod3gaYLAr"
      },
      "outputs": [],
      "source": [
        "dir = '/content/drive/MyDrive/Zero_Shot_DeepFake_Image_Classification/CLIP_based_deepfake_detection/dataset/FS'\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(dir, 'test'), transform=test_transform)\n",
        "test_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers= os.cpu_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuseiNQCYLAs"
      },
      "outputs": [],
      "source": [
        "# Test the model after training\n",
        "test_model(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4OI2qNcJ4H7"
      },
      "source": [
        "# **NT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez2YL8U9J4H7"
      },
      "outputs": [],
      "source": [
        "dir = '/content/drive/MyDrive/Zero_Shot_DeepFake_Image_Classification/CLIP_based_deepfake_detection/dataset/NT'\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(dir, 'test'), transform=test_transform)\n",
        "test_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers= os.cpu_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zp1t3YsJ4H8"
      },
      "outputs": [],
      "source": [
        "# Test the model after training\n",
        "test_model(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YAWPQs4KPtR"
      },
      "source": [
        "# **DF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nbWt6nIKPtS"
      },
      "outputs": [],
      "source": [
        "dir = '/content/drive/MyDrive/Zero_Shot_DeepFake_Image_Classification/CLIP_based_deepfake_detection/dataset/DF'\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(dir, 'test'), transform=test_transform)\n",
        "test_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers= os.cpu_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S62Djg4cKPtS"
      },
      "outputs": [],
      "source": [
        "# Test the model after training\n",
        "test_model(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHEX3FiHLUw4"
      },
      "source": [
        "# **DFD**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEaT19PTLUw5"
      },
      "outputs": [],
      "source": [
        "dir = '/content/drive/MyDrive/Zero_Shot_DeepFake_Image_Classification/CLIP_based_deepfake_detection/dataset/DFD'\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(dir, 'test'), transform=test_transform)\n",
        "test_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers= os.cpu_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ymn2NaBkLUw6"
      },
      "outputs": [],
      "source": [
        "# Test the model after training\n",
        "test_model(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UDHXxUbLbV2"
      },
      "source": [
        "# **F2F**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJN0h9azLbV3"
      },
      "outputs": [],
      "source": [
        "dir = '/content/drive/MyDrive/Zero_Shot_DeepFake_Image_Classification/CLIP_based_deepfake_detection/dataset/F2F'\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(dir, 'test'), transform=test_transform)\n",
        "test_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers= os.cpu_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ylv15-IPLbV4"
      },
      "outputs": [],
      "source": [
        "# Test the model after training\n",
        "test_model(model, test_loader, device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}