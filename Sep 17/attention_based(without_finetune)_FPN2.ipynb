{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/ICASSP_2025/without_finetune'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TkjOCZYOuPV",
        "outputId": "8f45bba3-be14-46b5-9a53-6e99137eb491"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Aerial_Scene_Recognition/ClassificationAfterFinetune/without_finetune\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "seed = 43\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "E_-10zkSRIFf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oU7da441Lnee"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "with h5py.File('train_val_vision_embeddings.h5', 'r') as f:\n",
        "    image_embeddings = f['train_val_vision_embeddings'][:]\n",
        "\n",
        "with h5py.File('train_val_audio_embeddings.h5', 'r') as f:\n",
        "    audio_embeddings = f['train_val_audio_embeddings'][:]\n",
        "\n",
        "labels = np.load('train_val_labels_inputs.npy')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_embeddings = torch.tensor(image_embeddings, dtype=torch.float32)\n",
        "audio_embeddings = torch.tensor(audio_embeddings, dtype=torch.float32)\n",
        "labels = torch.tensor(labels, dtype=torch.long)"
      ],
      "metadata": {
        "id": "a2Qu5ymOUXmm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = torch.utils.data.TensorDataset(image_embeddings, audio_embeddings, labels)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True) # original batch size = 32, but 16 is better"
      ],
      "metadata": {
        "id": "s3-lYuikUg_q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class FPN(nn.Module):\n",
        "    def __init__(self, input_dim, feature_size=256):\n",
        "        super(FPN, self).__init__()\n",
        "\n",
        "        self.lateral = nn.Conv1d(in_channels=1, out_channels=1,kernel_size=3,stride=1,padding=1)\n",
        "\n",
        "        self.smooth1 = nn.Conv1d(in_channels=1, out_channels=1,kernel_size=3,stride=1,padding=1)\n",
        "        self.smooth2 = nn.Conv1d(in_channels=1, out_channels=1,kernel_size=3,stride=1,padding=1)\n",
        "        self.smooth3 = nn.Conv1d(in_channels=1, out_channels=1,kernel_size=3,stride=1,padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print('x.shape:',x.shape)\n",
        "        p1 = self.lateral(x)\n",
        "        #print('p1.shape:',p1.shape)\n",
        "        p2 = nn.functional.avg_pool1d(p1, kernel_size=2, stride=2)\n",
        "        #print('p2.shape:',p2.shape)\n",
        "        p3 = nn.functional.avg_pool1d(p2, kernel_size=2,stride=2)\n",
        "        #print('p3.shape:',p3.shape)\n",
        "\n",
        "        p2_upsampled = nn.functional.interpolate(p2, size=p1.size(-1), mode='nearest')\n",
        "        #print('p2_upsampled.shape:',p2_upsampled.shape)\n",
        "        p3_upsampled = nn.functional.interpolate(p3, size=p1.size(-1), mode='nearest')\n",
        "        #print('p3_upsampled.shape:',p3_upsampled.shape)\n",
        "\n",
        "        p_combined = p1 + p2_upsampled + p3_upsampled\n",
        "        #print('p_combined.shape:',p_combined.shape)\n",
        "\n",
        "        p_combined = self.smooth1(p_combined)\n",
        "        p_combined = self.smooth2(p_combined)\n",
        "        p_combined = self.smooth3(p_combined)\n",
        "\n",
        "        return p_combined\n",
        "\n",
        "class DualAttentionWithFPN(nn.Module):\n",
        "    def __init__(self, input_dim, fpn_feature_size=1024, num_heads=8):\n",
        "        super(DualAttentionWithFPN, self).__init__()\n",
        "        self.fpn = FPN(input_dim, feature_size=fpn_feature_size)\n",
        "        self.image_attn = nn.MultiheadAttention(fpn_feature_size, num_heads)\n",
        "        self.audio_attn = nn.MultiheadAttention(fpn_feature_size, num_heads)\n",
        "        self.cross_modal_attn = nn.MultiheadAttention(fpn_feature_size, num_heads)\n",
        "        self.norm = nn.LayerNorm(fpn_feature_size)\n",
        "\n",
        "    def forward(self, image_embeddings, audio_embeddings):\n",
        "        image_features = self.fpn(image_embeddings)\n",
        "        audio_features = self.fpn(audio_embeddings)\n",
        "\n",
        "        img_attn_output, _ = self.image_attn(image_features, image_features, image_features)\n",
        "        img_attn_output = self.norm(img_attn_output + image_features)\n",
        "\n",
        "        audio_attn_output, _ = self.audio_attn(audio_features, audio_features, audio_features)\n",
        "        audio_attn_output = self.norm(audio_attn_output + audio_features)\n",
        "\n",
        "        combined_attn_output, _ = self.cross_modal_attn(img_attn_output, audio_attn_output, audio_attn_output)\n",
        "        combined_attn_output = self.norm(combined_attn_output + img_attn_output)\n",
        "\n",
        "        return combined_attn_output\n",
        "\n",
        "class TransformerClassifierWithFPN(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, fpn_feature_size=1024, num_heads=8, num_layers=2, dim_feedforward=2048, dropout=0.1):\n",
        "        super(TransformerClassifierWithFPN, self).__init__()\n",
        "        self.dual_attention_fpn = DualAttentionWithFPN(input_dim, fpn_feature_size, num_heads)\n",
        "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=fpn_feature_size, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=num_layers)\n",
        "        self.norm1 = nn.LayerNorm(fpn_feature_size)\n",
        "        self.fc = nn.Linear(fpn_feature_size, num_classes)\n",
        "\n",
        "    def forward(self, image_embeddings, audio_embeddings):\n",
        "        combined_embeddings = self.dual_attention_fpn(image_embeddings.unsqueeze(1), audio_embeddings.unsqueeze(1))\n",
        "\n",
        "        combined_embeddings = self.transformer_encoder(combined_embeddings)\n",
        "\n",
        "        combined_embeddings = self.norm1(combined_embeddings.mean(dim=1))\n",
        "\n",
        "        x = self.fc(combined_embeddings)\n",
        "\n",
        "        return x\n",
        "\n",
        "input_dim = image_embeddings.shape[1]\n",
        "num_classes = 13\n",
        "model = TransformerClassifierWithFPN(input_dim=input_dim, num_classes=num_classes, fpn_feature_size=1024)"
      ],
      "metadata": {
        "id": "nXRYbe7lL17l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ba7d04f-5886-4dcf-b37c-d9d8ce85f04b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "num_epochs = 5\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    batch_count = 0\n",
        "    for image_embeds, audio_embeds, targets in dataloader:\n",
        "        batch_count += 1\n",
        "        sys.stdout.write(f\"\\rBatch {batch_count}/{len(dataloader)}\")\n",
        "        sys.stdout.flush()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(image_embeds, audio_embeds)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss}\")\n"
      ],
      "metadata": {
        "id": "E3ZE7k9FL9ke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c41f8e1-7e07-4f53-83dc-7be678c669bc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 126/126Epoch 1, Average Loss: 2.5098656756537303\n",
            "Batch 126/126Epoch 2, Average Loss: 2.4186147886609275\n",
            "Batch 126/126Epoch 3, Average Loss: 2.405965216576107\n",
            "Batch 126/126Epoch 4, Average Loss: 2.403404854592823\n",
            "Batch 126/126Epoch 5, Average Loss: 2.4023543123214965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TESTING..**"
      ],
      "metadata": {
        "id": "bNF028DqObZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with h5py.File('test_vision_embeddings.h5', 'r') as f:\n",
        "    test_image_embeddings = f['test_vision_embeddings'][:]\n",
        "\n",
        "with h5py.File('test_audio_embeddings.h5', 'r') as f:\n",
        "    test_audio_embeddings = f['test_audio_embeddings'][:]\n",
        "\n",
        "test_labels = np.load('test_labels_inputs.npy')"
      ],
      "metadata": {
        "id": "S1fTNWulOeao"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_image_embeddings = l2_normalize(test_image_embeddings)\n",
        "# test_audio_embeddings = l2_normalize(test_audio_embeddings)"
      ],
      "metadata": {
        "id": "FvHFDVe3zBM6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "test_image_embeddings = torch.tensor(test_image_embeddings, dtype=torch.float32)\n",
        "test_audio_embeddings = torch.tensor(test_audio_embeddings, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test_labels, dtype=torch.long)"
      ],
      "metadata": {
        "id": "d9zHFStnW2_l"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = torch.utils.data.TensorDataset(test_image_embeddings, test_audio_embeddings, test_labels)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "4DSJEdnuXFYZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for image_embeds, audio_embeds, labels in test_dataloader:\n",
        "        outputs = model(image_embeds, audio_embeds)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "id": "MDdVoLtJXJCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b05deb0-35b8-4f47-92e9-17937ee74a6c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.0398\n",
            "Recall: 0.1994\n",
            "F1 Score: 0.0663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}