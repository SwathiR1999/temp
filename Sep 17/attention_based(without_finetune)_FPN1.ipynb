{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/ICASSP_2025/without_finetune'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TkjOCZYOuPV",
        "outputId": "a6b63e67-ef01-4d9e-ab1c-97485c2d5713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Aerial_Scene_Recognition/ClassificationAfterFinetune/without_finetune\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "seed = 43\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "E_-10zkSRIFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU7da441Lnee"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "# Load image embeddings\n",
        "with h5py.File('train_val_vision_embeddings.h5', 'r') as f:\n",
        "    image_embeddings = f['train_val_vision_embeddings'][:]\n",
        "\n",
        "# Load audio embeddings\n",
        "with h5py.File('train_val_audio_embeddings.h5', 'r') as f:\n",
        "    audio_embeddings = f['train_val_audio_embeddings'][:]\n",
        "\n",
        "# Load labels\n",
        "labels = np.load('train_val_labels_inputs.npy')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_embeddings = torch.tensor(image_embeddings, dtype=torch.float32)\n",
        "audio_embeddings = torch.tensor(audio_embeddings, dtype=torch.float32)\n",
        "labels = torch.tensor(labels, dtype=torch.long)"
      ],
      "metadata": {
        "id": "a2Qu5ymOUXmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = torch.utils.data.TensorDataset(image_embeddings, audio_embeddings, labels)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True) # original batch size = 32, but 16 is better"
      ],
      "metadata": {
        "id": "s3-lYuikUg_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import math\n",
        "\n",
        "# Dual Attention Mechanism with Feature Pyramid\n",
        "class DualAttentionWithFeaturePyramid(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads=8):\n",
        "        super(DualAttentionWithFeaturePyramid, self).__init__()\n",
        "        # Linear layers to create feature pyramid\n",
        "        self.project_512 = nn.Linear(embed_dim, 512)\n",
        "        self.project_256 = nn.Linear(512, 256)\n",
        "        self.project_128 = nn.Linear(256, 128)\n",
        "\n",
        "        self.image_attn = nn.MultiheadAttention(512, num_heads)\n",
        "        self.audio_attn = nn.MultiheadAttention(512, num_heads)\n",
        "        self.cross_modal_attn = nn.MultiheadAttention(512, num_heads)\n",
        "        self.norm = nn.LayerNorm(512)\n",
        "\n",
        "    def forward(self, image_embeddings, audio_embeddings):\n",
        "        # Create feature pyramid for image embeddings\n",
        "        img_pyramid_512 = self.project_512(image_embeddings)\n",
        "        img_pyramid_256 = self.project_256(img_pyramid_512)\n",
        "        img_pyramid_128 = self.project_128(img_pyramid_256)\n",
        "\n",
        "        # Create feature pyramid for audio embeddings\n",
        "        audio_pyramid_512 = self.project_512(audio_embeddings)\n",
        "        audio_pyramid_256 = self.project_256(audio_pyramid_512)\n",
        "        audio_pyramid_128 = self.project_128(audio_pyramid_256)\n",
        "\n",
        "        # Self-attention on image embeddings (at 512 dimension)\n",
        "        img_attn_output, _ = self.image_attn(img_pyramid_512, img_pyramid_512, img_pyramid_512)\n",
        "        img_attn_output = self.norm(img_attn_output + img_pyramid_512)\n",
        "\n",
        "        # Self-attention on audio embeddings (at 512 dimension)\n",
        "        audio_attn_output, _ = self.audio_attn(audio_pyramid_512, audio_pyramid_512, audio_pyramid_512)\n",
        "        audio_attn_output = self.norm(audio_attn_output + audio_pyramid_512)\n",
        "\n",
        "        # Cross-attention between image and audio embeddings (at 512 dimension)\n",
        "        combined_attn_output, _ = self.cross_modal_attn(img_attn_output, audio_attn_output, audio_attn_output)\n",
        "        combined_attn_output = self.norm(combined_attn_output + img_attn_output)\n",
        "\n",
        "        # Concatenate pyramid outputs (512, 256, 128 dimensions)\n",
        "        pyramid_combined = torch.cat([combined_attn_output, img_pyramid_256, img_pyramid_128], dim=-1)\n",
        "\n",
        "        return pyramid_combined\n",
        "\n",
        "\n",
        "# Transformer Classifier with Feature Pyramid\n",
        "class TransformerClassifierWithFeaturePyramid(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, num_heads=8, num_layers=2, dim_feedforward=2048, dropout=0.4):\n",
        "        super(TransformerClassifierWithFeaturePyramid, self).__init__()\n",
        "        self.dual_attention = DualAttentionWithFeaturePyramid(input_dim, num_heads)\n",
        "        # Adjusting the input dimension to account for concatenated pyramid features (512+256+128)\n",
        "        pyramid_dim = 512 + 256 + 128\n",
        "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=pyramid_dim, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=num_layers)\n",
        "        self.norm1 = nn.LayerNorm(pyramid_dim)\n",
        "        self.fc = nn.Linear(pyramid_dim, num_classes)\n",
        "\n",
        "    def forward(self, image_embeddings, audio_embeddings):\n",
        "        # Apply dual attention with feature pyramid\n",
        "        combined_embeddings = self.dual_attention(image_embeddings.unsqueeze(1), audio_embeddings.unsqueeze(1))\n",
        "\n",
        "        # Pass through transformer encoder\n",
        "        combined_embeddings = self.transformer_encoder(combined_embeddings)\n",
        "\n",
        "        # Apply normalization\n",
        "        combined_embeddings = self.norm1(combined_embeddings.mean(dim=1))\n",
        "\n",
        "        # Final classification layer\n",
        "        x = self.fc(combined_embeddings)\n",
        "\n",
        "        return x\n",
        "\n",
        "input_dim = image_embeddings.shape[1]  # Assuming image_embeddings and audio_embeddings have the same dimension\n",
        "num_classes = 13  # Number of classes\n",
        "model = TransformerClassifierWithFeaturePyramid(input_dim=input_dim, num_classes=num_classes)"
      ],
      "metadata": {
        "id": "nXRYbe7lL17l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73bf0e49-e021-4514-e268-15efdd11c393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6, weight_decay=1e-5)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    batch_count = 0\n",
        "    for image_embeds, audio_embeds, targets in dataloader:\n",
        "        batch_count += 1\n",
        "        sys.stdout.write(f\"\\rBatch {batch_count}/{len(dataloader)}\")\n",
        "        sys.stdout.flush()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(image_embeds, audio_embeds)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss}\")\n"
      ],
      "metadata": {
        "id": "E3ZE7k9FL9ke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f58bbfe-5894-420e-b847-315fe26c58a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1003/1003Epoch 1, Average Loss: 2.28280349517035\n",
            "Batch 1003/1003Epoch 2, Average Loss: 1.878329017167077\n",
            "Batch 1003/1003Epoch 3, Average Loss: 1.49750339155896\n",
            "Batch 1003/1003Epoch 4, Average Loss: 1.1704496658692212\n",
            "Batch 1003/1003Epoch 5, Average Loss: 0.9390005184521347\n",
            "Batch 1003/1003Epoch 6, Average Loss: 0.7807980781998616\n",
            "Batch 1003/1003Epoch 7, Average Loss: 0.6796127285693186\n",
            "Batch 1003/1003Epoch 8, Average Loss: 0.6003643114988879\n",
            "Batch 1003/1003Epoch 9, Average Loss: 0.5478417207695848\n",
            "Batch 1003/1003Epoch 10, Average Loss: 0.5024964632587146\n",
            "Batch 1003/1003Epoch 11, Average Loss: 0.46603669464432174\n",
            "Batch 1003/1003Epoch 12, Average Loss: 0.43203401896110916\n",
            "Batch 1003/1003Epoch 13, Average Loss: 0.4148990849751686\n",
            "Batch 1003/1003Epoch 14, Average Loss: 0.4006680487483324\n",
            "Batch 1003/1003Epoch 15, Average Loss: 0.37289680306106093\n",
            "Batch 1003/1003Epoch 16, Average Loss: 0.3609054411704822\n",
            "Batch 1003/1003Epoch 17, Average Loss: 0.3454882942608085\n",
            "Batch 1003/1003Epoch 18, Average Loss: 0.3292577240090531\n",
            "Batch 1003/1003Epoch 19, Average Loss: 0.31269231419601773\n",
            "Batch 1003/1003Epoch 20, Average Loss: 0.3059629408014959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TESTING..**"
      ],
      "metadata": {
        "id": "bNF028DqObZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test image embeddings\n",
        "with h5py.File('test_vision_embeddings.h5', 'r') as f:\n",
        "    test_image_embeddings = f['test_vision_embeddings'][:]\n",
        "\n",
        "# Load test audio embeddings\n",
        "with h5py.File('test_audio_embeddings.h5', 'r') as f:\n",
        "    test_audio_embeddings = f['test_audio_embeddings'][:]\n",
        "\n",
        "# Load test labels\n",
        "test_labels = np.load('test_labels_inputs.npy')\n",
        "\n",
        "# # Load test image embeddings\n",
        "# with h5py.File('test_vision_embeddings.h5', 'r') as f:\n",
        "#     test_image_embeddings = f['test_vision_embeddings'][:]\n",
        "\n",
        "# # Load test audio embeddings\n",
        "# with h5py.File('test_audio_embeddings.h5', 'r') as f:\n",
        "#     test_audio_embeddings = f['test_audio_embeddings'][:]\n",
        "\n",
        "# # Load test labels\n",
        "# test_labels = np.load('test_labels_inputs.npy')"
      ],
      "metadata": {
        "id": "S1fTNWulOeao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_image_embeddings = l2_normalize(test_image_embeddings)\n",
        "# test_audio_embeddings = l2_normalize(test_audio_embeddings)"
      ],
      "metadata": {
        "id": "FvHFDVe3zBM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "test_image_embeddings = torch.tensor(test_image_embeddings, dtype=torch.float32)\n",
        "test_audio_embeddings = torch.tensor(test_audio_embeddings, dtype=torch.float32)\n",
        "test_labels = torch.tensor(test_labels, dtype=torch.long)"
      ],
      "metadata": {
        "id": "d9zHFStnW2_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = torch.utils.data.TensorDataset(test_image_embeddings, test_audio_embeddings, test_labels)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "4DSJEdnuXFYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize lists to store true labels and predictions\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# No gradient calculation needed during evaluation\n",
        "with torch.no_grad():\n",
        "    for image_embeds, audio_embeds, labels in test_dataloader:\n",
        "        # Forward pass\n",
        "        outputs = model(image_embeds, audio_embeds)\n",
        "\n",
        "        # Get predicted class\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        # Append predictions and true labels to lists\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "id": "MDdVoLtJXJCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97377ffe-597f-4840-fd7e-2e8b04596315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9343\n",
            "Recall: 0.9313\n",
            "F1 Score: 0.9276\n"
          ]
        }
      ]
    }
  ]
}