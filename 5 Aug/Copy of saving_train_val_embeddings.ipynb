{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V100","mount_file_id":"1qqS-fp-L8q_FHIejhkijvYt1kv4UOxoe","authorship_tag":"ABX9TyMTycUH0Z6ZU/0IdB9v6g9z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bIVjkL7PFfmw","executionInfo":{"status":"ok","timestamp":1722789911982,"user_tz":-330,"elapsed":1417,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}},"outputId":"d1e16c20-4b41-4db6-fe58-f62e9104f68c"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Aerial_Scene_Recognition\n"]}],"source":["%cd /content/drive/MyDrive/Aerial_Scene_Recognition"]},{"cell_type":"code","source":["import logging\n","logging.basicConfig(level=logging.INFO, force=True)"],"metadata":{"id":"WUMFL22csHmn","executionInfo":{"status":"ok","timestamp":1722789911982,"user_tz":-330,"elapsed":4,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!pip install -r requirements.txt --quiet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"QvSH64-CGEcN","executionInfo":{"status":"ok","timestamp":1722790144146,"user_tz":-330,"elapsed":232168,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}},"outputId":"346b0cd2-6395-4ca8-89c3-82373cd128ca"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.1/890.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.2/866.2 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.0/230.0 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.9/268.9 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.3/802.3 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for mayavi (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 1.4.12 requires pydantic>=2.7.0, but you have pydantic 1.10.17 which is incompatible.\n","gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2023.12.2 which is incompatible.\n","torchtext 0.18.0 requires torch>=2.3.0, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["import os\n","current_directory = os.getcwd()"],"metadata":{"id":"_HVdnIuEGIMr","executionInfo":{"status":"ok","timestamp":1722790144146,"user_tz":-330,"elapsed":12,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import torch\n","from models import imagebind_model\n","from models.imagebind_model import ModalityType, load_module\n","from models import lora as LoRA\n","\n","device = \"cpu\""],"metadata":{"id":"XBmwlAubGMk3","executionInfo":{"status":"ok","timestamp":1722790151064,"user_tz":-330,"elapsed":6928,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["model = imagebind_model.imagebind_huge(pretrained=True)"],"metadata":{"id":"t0Hmdfbypzxg","executionInfo":{"status":"ok","timestamp":1722790288506,"user_tz":-330,"elapsed":137450,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["lora = True\n","linear_probing = False\n","load_head_post_proc_finetuned = True"],"metadata":{"id":"3GlTssmSfXKP","executionInfo":{"status":"ok","timestamp":1722790288507,"user_tz":-330,"elapsed":9,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["if lora:\n","    model.modality_trunks.update(\n","        LoRA.apply_lora_modality_trunks(model.modality_trunks, rank=4,\n","                                        modality_names=[ModalityType.VISION, ModalityType.AUDIO]))\n","\n","    LoRA.load_lora_modality_trunks(model.modality_trunks,\n","                                   checkpoint_dir=\".checkpoints/lora/\", postfix=\"_last\")\n","\n","    if load_head_post_proc_finetuned:\n","        load_module(model.modality_postprocessors, module_name=\"postprocessors\",\n","                    checkpoint_dir=\".checkpoints/lora/\", postfix=\"_last\")\n","        load_module(model.modality_heads, module_name=\"heads\",\n","                    checkpoint_dir=\".checkpoints/lora/\", postfix=\"_last\")\n","elif linear_probing:\n","    load_module(model.modality_heads, module_name=\"heads\",\n","                checkpoint_dir=\"./.checkpoints/lora/\", postfix=\"_last\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ek901bkZfP6F","executionInfo":{"status":"ok","timestamp":1722790298665,"user_tz":-330,"elapsed":10165,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}},"outputId":"8c40dfe1-8722-4fda-e4e6-b6107d665e65"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:root:Loaded LoRA parameters for modality vision from .checkpoints/lora/.\n","INFO:root:Loaded LoRA parameters for modality audio from .checkpoints/lora/.\n","INFO:root:Loaded parameters for module postprocessors from .checkpoints/lora/.\n","INFO:root:Loaded parameters for module heads from .checkpoints/lora/.\n"]}]},{"cell_type":"code","source":["model.eval()\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"K5iQMGLTslu1","executionInfo":{"status":"ok","timestamp":1722790298665,"user_tz":-330,"elapsed":95,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}},"outputId":"8e5c741b-b984-480a-b3de-02122e1827fe"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ImageBindModel(\n","  (modality_preprocessors): ModuleDict(\n","    (vision): RGBDTPreprocessor(\n","      (cls_token): tensor((1, 1, 1280), requires_grad=True)\n","      \n","      (rgbt_stem): PatchEmbedGeneric(\n","        (proj): Sequential(\n","          (0): PadIm2Video()\n","          (1): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n","        )\n","      )\n","      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n","        (pos_embed): tensor((1, 257, 1280), requires_grad=True)\n","        \n","      )\n","    )\n","    (text): TextPreprocessor(\n","      (pos_embed): tensor((1, 77, 1024), requires_grad=True)\n","      (mask): tensor((77, 77), requires_grad=False)\n","      \n","      (token_embedding): Embedding(49408, 1024)\n","    )\n","    (audio): AudioPreprocessor(\n","      (cls_token): tensor((1, 1, 768), requires_grad=True)\n","      \n","      (rgbt_stem): PatchEmbedGeneric(\n","        (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10), bias=False)\n","        (norm_layer): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n","        (pos_embed): tensor((1, 229, 768), requires_grad=True)\n","        \n","      )\n","    )\n","    (depth): RGBDTPreprocessor(\n","      (cls_token): tensor((1, 1, 384), requires_grad=True)\n","      \n","      (depth_stem): PatchEmbedGeneric(\n","        (proj): Conv2d(1, 384, kernel_size=(16, 16), stride=(16, 16), bias=False)\n","        (norm_layer): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n","        (pos_embed): tensor((1, 197, 384), requires_grad=True)\n","        \n","      )\n","    )\n","    (thermal): ThermalPreprocessor(\n","      (cls_token): tensor((1, 1, 768), requires_grad=True)\n","      \n","      (rgbt_stem): PatchEmbedGeneric(\n","        (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n","        (norm_layer): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n","        (pos_embed): tensor((1, 197, 768), requires_grad=True)\n","        \n","      )\n","    )\n","    (imu): IMUPreprocessor(\n","      (pos_embed): tensor((1, 251, 512), requires_grad=True)\n","      (cls_token): tensor((1, 1, 512), requires_grad=True)\n","      \n","      (imu_stem): PatchEmbedGeneric(\n","        (proj): Linear(in_features=48, out_features=512, bias=False)\n","        (norm_layer): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (modality_trunks): ModuleDict(\n","    (vision): LoRA_SimpleTransformer(\n","      (lora_model): SimpleTransformer(\n","        (pre_transformer_layer): Sequential(\n","          (0): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","          (1): EinOpsRearrange()\n","        )\n","        (blocks): Sequential(\n","          (0): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (1): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (2): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (3): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (4): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (5): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (6): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (7): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (8): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (9): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (10): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (11): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (12): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (13): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (14): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (15): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (16): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (17): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (18): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (19): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (20): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (21): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (22): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (23): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (24): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (25): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (26): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (27): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (28): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (29): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (30): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","          (31): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","              )\n","              (w_a): Linear(in_features=1280, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=1280, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","            )\n","          )\n","        )\n","        (post_transformer_layer): EinOpsRearrange()\n","      )\n","    )\n","    (text): SimpleTransformer(\n","      (pre_transformer_layer): Sequential(\n","        (0): Identity()\n","        (1): EinOpsRearrange()\n","      )\n","      (blocks): Sequential(\n","        (0): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (1): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (2): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (3): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (4): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (5): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (6): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (7): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (8): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (9): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (10): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (11): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (12): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (13): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (14): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (15): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (16): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (17): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (18): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (19): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (20): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (21): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (22): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (23): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        )\n","      )\n","      (post_transformer_layer): EinOpsRearrange()\n","    )\n","    (audio): LoRA_SimpleTransformer(\n","      (lora_model): SimpleTransformer(\n","        (pre_transformer_layer): Sequential(\n","          (0): Identity()\n","          (1): EinOpsRearrange()\n","        )\n","        (blocks): Sequential(\n","          (0): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","              )\n","              (w_a): Linear(in_features=768, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=768, bias=False)\n","            )\n","            (drop_path): Identity()\n","            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","          )\n","          (1): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","              )\n","              (w_a): Linear(in_features=768, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=768, bias=False)\n","            )\n","            (drop_path): DropPath(drop_prob=0.009)\n","            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","          )\n","          (2): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","              )\n","              (w_a): Linear(in_features=768, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=768, bias=False)\n","            )\n","            (drop_path): DropPath(drop_prob=0.018)\n","            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","          )\n","          (3): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","              )\n","              (w_a): Linear(in_features=768, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=768, bias=False)\n","            )\n","            (drop_path): DropPath(drop_prob=0.027)\n","            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","          )\n","          (4): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","              )\n","              (w_a): Linear(in_features=768, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=768, bias=False)\n","            )\n","            (drop_path): DropPath(drop_prob=0.036)\n","            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","          )\n","          (5): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","              )\n","              (w_a): Linear(in_features=768, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=768, bias=False)\n","            )\n","            (drop_path): DropPath(drop_prob=0.045)\n","            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","          )\n","          (6): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","              )\n","              (w_a): Linear(in_features=768, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=768, bias=False)\n","            )\n","            (drop_path): DropPath(drop_prob=0.055)\n","            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","          )\n","          (7): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","              )\n","              (w_a): Linear(in_features=768, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=768, bias=False)\n","            )\n","            (drop_path): DropPath(drop_prob=0.064)\n","            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","          )\n","          (8): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","              )\n","              (w_a): Linear(in_features=768, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=768, bias=False)\n","            )\n","            (drop_path): DropPath(drop_prob=0.073)\n","            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","          )\n","          (9): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","              )\n","              (w_a): Linear(in_features=768, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=768, bias=False)\n","            )\n","            (drop_path): DropPath(drop_prob=0.082)\n","            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","          )\n","          (10): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","              )\n","              (w_a): Linear(in_features=768, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=768, bias=False)\n","            )\n","            (drop_path): DropPath(drop_prob=0.091)\n","            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","          )\n","          (11): BlockWithMasking(\n","            (attn): _LoRALayer(\n","              (w): MultiheadAttention(\n","                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","              )\n","              (w_a): Linear(in_features=768, out_features=4, bias=False)\n","              (w_b): Linear(in_features=4, out_features=768, bias=False)\n","            )\n","            (drop_path): DropPath(drop_prob=0.100)\n","            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","              (act): GELU(approximate='none')\n","              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","            (prev_attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","          )\n","        )\n","        (post_transformer_layer): EinOpsRearrange()\n","      )\n","    )\n","    (depth): SimpleTransformer(\n","      (pre_transformer_layer): Sequential(\n","        (0): Identity()\n","        (1): EinOpsRearrange()\n","      )\n","      (blocks): Sequential(\n","        (0): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (1): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (2): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (3): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (4): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (5): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (6): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (7): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (8): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (9): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (10): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (11): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        )\n","      )\n","      (post_transformer_layer): EinOpsRearrange()\n","    )\n","    (thermal): SimpleTransformer(\n","      (pre_transformer_layer): Sequential(\n","        (0): Identity()\n","        (1): EinOpsRearrange()\n","      )\n","      (blocks): Sequential(\n","        (0): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (1): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (2): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (3): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (4): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (5): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (6): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (7): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (8): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (9): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (10): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (11): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","        )\n","      )\n","      (post_transformer_layer): EinOpsRearrange()\n","    )\n","    (imu): SimpleTransformer(\n","      (pre_transformer_layer): Sequential(\n","        (0): Identity()\n","        (1): EinOpsRearrange()\n","      )\n","      (blocks): Sequential(\n","        (0): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (drop_path): Identity()\n","          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (1): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (drop_path): DropPath(drop_prob=0.140)\n","          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (2): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (drop_path): DropPath(drop_prob=0.280)\n","          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (3): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (drop_path): DropPath(drop_prob=0.420)\n","          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (4): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (drop_path): DropPath(drop_prob=0.560)\n","          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        )\n","        (5): BlockWithMasking(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (drop_path): DropPath(drop_prob=0.700)\n","          (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","          (mlp): Mlp(\n","            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","            (act): GELU(approximate='none')\n","            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","            (drop): Dropout(p=0.0, inplace=False)\n","          )\n","          (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","        )\n","      )\n","      (post_transformer_layer): EinOpsRearrange()\n","    )\n","  )\n","  (modality_heads): ModuleDict(\n","    (vision): Sequential(\n","      (0): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n","      (1): SelectElement()\n","      (2): Linear(in_features=1280, out_features=1024, bias=False)\n","    )\n","    (text): SelectEOSAndProject(\n","      (proj): Sequential(\n","        (0): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n","        (1): Linear(in_features=1024, out_features=1024, bias=False)\n","      )\n","    )\n","    (audio): Sequential(\n","      (0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (1): SelectElement()\n","      (2): Linear(in_features=768, out_features=1024, bias=False)\n","    )\n","    (depth): Sequential(\n","      (0): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (1): SelectElement()\n","      (2): Linear(in_features=384, out_features=1024, bias=False)\n","    )\n","    (thermal): Sequential(\n","      (0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (1): SelectElement()\n","      (2): Linear(in_features=768, out_features=1024, bias=False)\n","    )\n","    (imu): Sequential(\n","      (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n","      (1): SelectElement()\n","      (2): Dropout(p=0.5, inplace=False)\n","      (3): Linear(in_features=512, out_features=1024, bias=False)\n","    )\n","  )\n","  (modality_postprocessors): ModuleDict(\n","    (vision): Normalize()\n","    (text): Sequential(\n","      (0): Normalize()\n","      (1): LearnableLogitScaling(logit_scale_init=14.285714285714285,learnable=True, max_logit_scale=100)\n","    )\n","    (audio): Sequential(\n","      (0): Normalize()\n","      (1): LearnableLogitScaling(logit_scale_init=20.0,learnable=False, max_logit_scale=100)\n","    )\n","    (depth): Sequential(\n","      (0): Normalize()\n","      (1): LearnableLogitScaling(logit_scale_init=5.0,learnable=False, max_logit_scale=100)\n","    )\n","    (thermal): Sequential(\n","      (0): Normalize()\n","      (1): LearnableLogitScaling(logit_scale_init=10.0,learnable=False, max_logit_scale=100)\n","    )\n","    (imu): Sequential(\n","      (0): Normalize()\n","      (1): LearnableLogitScaling(logit_scale_init=5.0,learnable=False, max_logit_scale=100)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["loaded_inputs = torch.load('train_val_inputs.pth', map_location=torch.device('cpu'))"],"metadata":{"id":"MosWYH5X85Xx","executionInfo":{"status":"ok","timestamp":1722790425633,"user_tz":-330,"elapsed":127029,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["loaded_inputs['vision'].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6sWwQDuA-fUf","executionInfo":{"status":"ok","timestamp":1722790425633,"user_tz":-330,"elapsed":38,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}},"outputId":"626a111d-6e46-4fe9-bff6-2803e74e5b35"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4012, 3, 224, 224])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["loaded_inputs['audio'].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-pY2-pDE-hi3","executionInfo":{"status":"ok","timestamp":1722790425634,"user_tz":-330,"elapsed":35,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}},"outputId":"5ab670c9-b1ac-47a1-e998-6c53cbe57c8c"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4012, 3, 1, 128, 204])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["import h5py\n","with h5py.File('ClassificationAfterFinetune/train_val_vision_embeddings.h5', 'w') as h5f:\n","  num_embeddings = loaded_inputs['vision'].shape[0]\n","  embedding_size = 1024\n","  dataset = h5f.create_dataset('ClassificationAfterFinetune/train_val_vision_embeddings', (num_embeddings, embedding_size), dtype='f')\n","  for i in range(num_embeddings):\n","        embd = model({'vision': torch.unsqueeze(loaded_inputs['vision'][i], dim=0)})\n","        dataset[i] = embd['vision'].detach().cpu().numpy()\n","        del embd"],"metadata":{"id":"GkTywNMy9CZk","executionInfo":{"status":"error","timestamp":1722798598730,"user_tz":-330,"elapsed":8173129,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}},"colab":{"base_uri":"https://localhost:8080/","height":408},"outputId":"b5e8c6ad-39dc-43c9-944f-ca7b884ce52c"},"execution_count":13,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-033c7edadda1>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0membd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'vision'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vision'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vision'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Aerial_Scene_Recognition/models/imagebind_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0mtrunk_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodality_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"trunk\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                 \u001b[0mhead_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodality_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"head\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m                 \u001b[0mmodality_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodality_trunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodality_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrunk_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m                 modality_value = self.modality_heads[modality_key](\n\u001b[1;32m    464\u001b[0m                     \u001b[0mmodality_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhead_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Aerial_Scene_Recognition/models/lora.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlora_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Aerial_Scene_Recognition/models/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens, attn_mask, use_checkpoint, checkpoint_every_n, checkpoint_blk_ids)\u001b[0m\n\u001b[1;32m    275\u001b[0m                 )\n\u001b[1;32m    276\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_transformer_layer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_transformer_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Aerial_Scene_Recognition/models/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_scale_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Aerial_Scene_Recognition/models/lora.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Aerial_Scene_Recognition/models/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMultiheadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiheadAttention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[1;32m   1165\u001b[0m                 v_proj_weight=self.v_proj_weight, average_attn_weights=average_attn_weights)\n\u001b[1;32m   1166\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1168\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[1;32m   5166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5167\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_len\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5168\u001b[0;31m     \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5169\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import h5py\n","with h5py.File('ClassificationAfterFinetune/train_val_audio_embeddings.h5', 'w') as h5f:\n","  num_embeddings = loaded_inputs['audio'].shape[0]\n","  embedding_size = 1024\n","  dataset = h5f.create_dataset('ClassificationAfterFinetune/train_val_audio_embeddings', (num_embeddings, embedding_size), dtype='f')\n","  for i in range(num_embeddings):\n","        embd = model({'audio': torch.unsqueeze(loaded_inputs['audio'][i], dim=0)})\n","        dataset[i] = embd['audio'].detach().cpu().numpy()\n","        del embd"],"metadata":{"id":"IU3seR_E9N-T"},"execution_count":null,"outputs":[]}]}