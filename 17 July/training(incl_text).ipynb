{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "HsUQdXVSF1iF",
        "outputId": "6692b678-e352-4ffd-dd9a-46f92d99ad39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ImageBind finetune/ImageBind-LoRA\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/ImageBind finetune/ImageBind-LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "4XfQxWPQW5yc",
        "outputId": "dc1cac39-6754-4945-a520-ada1c25ac9a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.1/890.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m969.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.3/812.3 kB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.0/230.0 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.9/268.9 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.3/812.3 kB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.3/812.3 kB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.2/812.2 kB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.3/802.3 kB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for mayavi (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt --quiet\n",
        "!pip install pytorch_lightning --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GiIIavf9auBB"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.INFO, force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Cnom_TIIzzrL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "num_workers = os.cpu_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "S_iFuw-ubRHV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2f522cf3-f3e1-4c9f-d0e5-db669f7c3073"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 43\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from pytorch_lightning import seed_everything\n",
        "seed_everything(43, workers=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "BUeblJ_HWfnr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2Tkbof-myceK",
        "outputId": "e4a07828-9000-41ce-f247-bdadb06b8f2f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "m2Bl0lhnaTpF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from models import imagebind_model\n",
        "from models import lora as LoRA\n",
        "from models.imagebind_model import ModalityType, load_module, save_module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSUZUUcSKr8P"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as L\n",
        "from pytorch_lightning import Trainer, seed_everything\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning import loggers as pl_loggers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FsSY0WAkcMmi"
      },
      "outputs": [],
      "source": [
        "self_contrast = False\n",
        "batch_size = 8\n",
        "num_workers= os.cpu_count()\n",
        "lora_modality_names_123 = [\"vision\", \"audio\", \"text\"]\n",
        "LOG_ON_STEP = False\n",
        "LOG_ON_EPOCH = True\n",
        "lora= True\n",
        "full_model_checkpointing = False\n",
        "full_model_checkpoint_dir=\"./.checkpoints/full\"\n",
        "lora_checkpoint_dir=\"./.checkpoints/lora\"\n",
        "device_name=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "max_epochs = 5\n",
        "gradient_clip_val=1.0\n",
        "loggers = None\n",
        "linear_probing = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PdVZoWh4LT86"
      },
      "outputs": [],
      "source": [
        "class ImageBindTrain(L.LightningModule):\n",
        "    def __init__(self, lr=5e-4, weight_decay=1e-4, max_epochs=500, batch_size=32, num_workers=4, seed=42,\n",
        "                 self_contrast=False, temperature=0.07,  momentum_betas=(0.9, 0.95),\n",
        "                 lora=False, lora_rank=4, lora_checkpoint_dir=\"./.checkpoints/lora\",\n",
        "                 lora_layer_idxs=None, lora_modality_names=None,\n",
        "                 linear_probing=False\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        assert not (linear_probing and lora), \\\n",
        "            \"Linear probing is a subset of LoRA training procedure for ImageBind. \" \\\n",
        "            \"Cannot set both linear_probing=True and lora=True. \" \\\n",
        "            \"Linear probing stores params in lora_checkpoint_dir\"\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # Load full pretrained ImageBind model\n",
        "        self.model = imagebind_model.imagebind_huge(pretrained=True)\n",
        "        if lora:\n",
        "            for modality_preprocessor in self.model.modality_preprocessors.children():\n",
        "                modality_preprocessor.requires_grad_(False)\n",
        "            for modality_trunk in self.model.modality_trunks.children():\n",
        "                modality_trunk.requires_grad_(False)\n",
        "\n",
        "            self.model.modality_trunks.update(LoRA.apply_lora_modality_trunks(self.model.modality_trunks, rank=lora_rank,\n",
        "                                                                              layer_idxs=lora_layer_idxs,\n",
        "                                                                              modality_names=lora_modality_names))\n",
        "            LoRA.load_lora_modality_trunks(self.model.modality_trunks, checkpoint_dir=lora_checkpoint_dir)\n",
        "\n",
        "            # Load postprocessors & heads\n",
        "            load_module(self.model.modality_postprocessors, module_name=\"postprocessors\",\n",
        "                        checkpoint_dir=lora_checkpoint_dir)\n",
        "            load_module(self.model.modality_heads, module_name=\"heads\",\n",
        "                        checkpoint_dir=lora_checkpoint_dir)\n",
        "        elif linear_probing:\n",
        "            for modality_preprocessor in self.model.modality_preprocessors.children():\n",
        "                modality_preprocessor.requires_grad_(False)\n",
        "            for modality_trunk in self.model.modality_trunks.children():\n",
        "                modality_trunk.requires_grad_(False)\n",
        "            for modality_postprocessor in self.model.modality_postprocessors.children():\n",
        "                modality_postprocessor.requires_grad_(False)\n",
        "\n",
        "            load_module(self.model.modality_heads, module_name=\"heads\",\n",
        "                        checkpoint_dir=lora_checkpoint_dir)\n",
        "            for modality_head in self.model.modality_heads.children():\n",
        "                modality_head.requires_grad_(False)\n",
        "                final_layer = list(modality_head.children())[-1]\n",
        "                final_layer.requires_grad_(True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay,\n",
        "                                betas=self.hparams.momentum_betas)\n",
        "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer, T_max=self.hparams.max_epochs, eta_min=self.hparams.lr / 50\n",
        "        )\n",
        "        return [optimizer], [lr_scheduler]\n",
        "\n",
        "    def info_nce_loss(self, batch, mode=\"train\"):\n",
        "      data_a, class_a, data_b, class_b, data_c, class_c = batch\n",
        "\n",
        "      # class_a is always \"vision\" according to ImageBind\n",
        "      feats_a = [self.model({class_a[0]: data_a_i}) for data_a_i in data_a]\n",
        "      feats_a_tensor = torch.cat([list(dict_.values())[0] for dict_ in feats_a], dim=0)\n",
        "\n",
        "      # class_b is always \"audio\"\n",
        "      feats_b = [self.model({class_b[0]: data_b_i}) for data_b_i in data_b]\n",
        "      feats_b_tensor = torch.cat([list(dict_.values())[0] for dict_ in feats_b], dim=0)\n",
        "\n",
        "      # class_c is always \"text\"\n",
        "      feats_c = [self.model({class_c[0]: data_c_i}) for data_c_i in data_c]\n",
        "      feats_c_tensor = torch.cat([list(dict_.values())[0] for dict_ in feats_c], dim=0)\n",
        "\n",
        "      if self.hparams.self_contrast:\n",
        "          feats_a_b_c_tensor = torch.cat([feats_a_tensor.chunk(3)[0], feats_b_tensor, feats_c_tensor], dim=0)\n",
        "          feats_tensors = [feats_a_tensor, feats_a_b_c_tensor]\n",
        "          temperatures = [1, self.hparams.temperature]\n",
        "          contrast = [\"self\", \"cross\"]\n",
        "      else:\n",
        "          feats_a_b_c_tensor = torch.cat([feats_a_tensor, feats_b_tensor, feats_c_tensor], dim=0)\n",
        "          feats_tensors = [feats_a_b_c_tensor]\n",
        "          temperatures = [self.hparams.temperature]\n",
        "          contrast = [\"cross\"]\n",
        "\n",
        "      dual_nll = False\n",
        "      for feats_idx, feats_tensor in enumerate(feats_tensors):\n",
        "          cos_sim = F.cosine_similarity(feats_tensor[:, None, :], feats_tensor[None, :, :], dim=-1)\n",
        "          self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool, device=cos_sim.device)\n",
        "          cos_sim.masked_fill_(self_mask, -9e15)\n",
        "          #pos_mask = self_mask.roll(shifts=cos_sim.shape[0] // 3, dims=0)\n",
        "          pos_mask_1 = self_mask.roll(shifts=cos_sim.shape[0]//3, dims=0)\n",
        "          pos_mask_2 = self_mask.roll(shifts=2 * cos_sim.shape[0]//3, dims=0)\n",
        "          pos_mask = pos_mask_1 | pos_mask_2\n",
        "          cos_sim = cos_sim / temperatures[feats_idx]\n",
        "          nll = -cos_sim[pos_mask] + torch.logsumexp(cos_sim, dim=-1)\n",
        "          nll = nll.mean()\n",
        "          if not dual_nll:\n",
        "              dual_nll = nll\n",
        "          else:\n",
        "              dual_nll += nll\n",
        "              dual_nll /= 2\n",
        "          self.log(mode + \"_loss_\" + contrast[feats_idx], nll, prog_bar=True,\n",
        "                  on_step=LOG_ON_STEP, on_epoch=LOG_ON_EPOCH, batch_size=self.hparams.batch_size)\n",
        "          comb_sim = torch.cat(\n",
        "              [cos_sim[pos_mask][:, None], cos_sim.masked_fill(pos_mask, -9e15)],\n",
        "              dim=-1,\n",
        "          )\n",
        "          sim_argsort = comb_sim.argsort(dim=-1, descending=True).argmin(dim=-1)\n",
        "          self.log(mode + \"_acc_top1\", (sim_argsort == 0).float().mean(), prog_bar=True,\n",
        "                  on_step=LOG_ON_STEP, on_epoch=LOG_ON_EPOCH, batch_size=self.hparams.batch_size)\n",
        "          self.log(mode + \"_acc_top5\", (sim_argsort < 5).float().mean(), prog_bar=True,\n",
        "                  on_step=LOG_ON_STEP, on_epoch=LOG_ON_EPOCH, batch_size=self.hparams.batch_size)\n",
        "          self.log(mode + \"_acc_mean_pos\", 1 + sim_argsort.float().mean(), prog_bar=True,\n",
        "                  on_step=LOG_ON_STEP, on_epoch=LOG_ON_EPOCH, batch_size=self.hparams.batch_size)\n",
        "\n",
        "      self.log(mode + \"_loss\", dual_nll, prog_bar=True,\n",
        "              on_step=LOG_ON_STEP, on_epoch=LOG_ON_EPOCH, batch_size=self.hparams.batch_size)\n",
        "      return dual_nll\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self.info_nce_loss(batch, mode=\"train\")\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        self.info_nce_loss(batch, mode=\"val\")\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        if self.hparams.lora:\n",
        "            # Save LoRA checkpoint\n",
        "            LoRA.save_lora_modality_trunks(self.model.modality_trunks, checkpoint_dir=self.hparams.lora_checkpoint_dir)\n",
        "            # Save postprocessors & heads\n",
        "            save_module(self.model.modality_postprocessors, module_name=\"postprocessors\",\n",
        "                        checkpoint_dir=self.hparams.lora_checkpoint_dir)\n",
        "            save_module(self.model.modality_heads, module_name=\"heads\",\n",
        "                        checkpoint_dir=self.hparams.lora_checkpoint_dir)\n",
        "        elif self.hparams.linear_probing:\n",
        "            # Save postprocessors & heads\n",
        "            save_module(self.model.modality_heads, module_name=\"heads\",\n",
        "                        checkpoint_dir=self.hparams.lora_checkpoint_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7VUwzDZeL9XG"
      },
      "outputs": [],
      "source": [
        "class ImageAudioDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, split='train', train_size=0.9, random_seed=42, device='cpu'):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.device = device\n",
        "\n",
        "        self.classes = [d for d in os.listdir(os.path.join(root_dir, 'images')) if os.path.isdir(os.path.join(root_dir, 'images', d))]\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "\n",
        "        self.image_paths = []\n",
        "        self.audio_paths = []\n",
        "        for cls in self.classes:\n",
        "            cls_image_dir = os.path.join(root_dir, 'images', cls)\n",
        "            cls_audio_dir = os.path.join(root_dir, 'audio', cls)\n",
        "            for filename in os.listdir(cls_image_dir):\n",
        "                filename_temp=filename[:-4]\n",
        "                if filename_temp[:-4] == \".DS_S\":\n",
        "                  continue\n",
        "                self.image_paths.append((os.path.join(cls_image_dir, filename_temp+\".jpg\"), cls))\n",
        "                self.audio_paths.append((os.path.join(cls_audio_dir, filename_temp+\".wav\"), cls))\n",
        "\n",
        "        # Split dataset\n",
        "        self.train_image_paths, self.test_image_paths = train_test_split(self.image_paths, train_size=train_size, random_state=random_seed)\n",
        "        self.train_audio_paths, self.test_audio_paths = train_test_split(self.audio_paths, train_size=train_size, random_state=random_seed)\n",
        "\n",
        "        if split == 'train':\n",
        "            self.image_paths = self.train_image_paths\n",
        "            self.audio_paths = self.train_audio_paths\n",
        "        elif split == 'test':\n",
        "            self.image_paths = self.test_image_paths\n",
        "            self.audio_paths = self.test_audio_paths\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid split argument. Expected 'train' or 'test', got {split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.image_paths), len(self.audio_paths))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path, class_text = self.image_paths[index]\n",
        "        audio_path, _ = self.audio_paths[index]\n",
        "        # Load and transform image\n",
        "        images = data.load_and_transform_vision_data([img_path], self.device, to_tensor=False)\n",
        "        if self.transform is not None:\n",
        "            image = images[0]\n",
        "            images = self.transform(image)\n",
        "\n",
        "        # Load and transform audio\n",
        "        audios = data.load_and_transform_audio_data([audio_path], self.device)\n",
        "\n",
        "        # Load and transform text\n",
        "        texts = data.load_and_transform_text([class_text], self.device)\n",
        "\n",
        "        return images, ModalityType.VISION, audios, ModalityType.AUDIO, texts, ModalityType.TEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "j2hswaVqZXU7"
      },
      "outputs": [],
      "source": [
        "contrast_transforms = transforms.Compose(\n",
        "        [\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomResizedCrop(size=224),\n",
        "            transforms.RandomApply([transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1)],\n",
        "                                   p=0.8),\n",
        "            transforms.RandomGrayscale(p=0.2),\n",
        "            transforms.GaussianBlur(kernel_size=9),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "                std=(0.26862954, 0.26130258, 0.27577711),\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "class ContrastiveTransformations:\n",
        "    def __init__(self, base_transforms, n_views=2):\n",
        "        self.base_transforms = base_transforms\n",
        "        self.n_views = n_views\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return [self.base_transforms(x) for _ in range(self.n_views)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kPzhm20QbdQk"
      },
      "outputs": [],
      "source": [
        "train_datasets = []\n",
        "test_datasets = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vZtqEVnZbBxh"
      },
      "outputs": [],
      "source": [
        "train_datasets.append(ImageAudioDataset(\n",
        "            root_dir=os.getcwd()+\"/new_data/\", split=\"train\",\n",
        "            transform=ContrastiveTransformations(contrast_transforms,\n",
        "                                                 n_views=2 if self_contrast else 1)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_datasets.append(ImageAudioDataset(\n",
        "            root_dir=os.getcwd()+\"/new_data/\", split=\"test\",\n",
        "            transform=ContrastiveTransformations(contrast_transforms,\n",
        "                                                 n_views=2 if self_contrast else 1)))"
      ],
      "metadata": {
        "id": "-oe9CAJAu-O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JY6C-s3kuRVP"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_datasets[0]\n",
        "test_dataset = test_datasets[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM4rpI7swq5d"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        pin_memory=False,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "val_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "        pin_memory=False,\n",
        "        num_workers=num_workers,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "F1ODf0-LGhxj"
      },
      "outputs": [],
      "source": [
        "lora_layer_idxs = {}\n",
        "lora_modality_names = []\n",
        "modalities = [\"vision\", \"text\", \"audio\", \"thermal\", \"depth\", \"imu\"]\n",
        "for modality_name in lora_modality_names_123:\n",
        "    if modality_name in modalities:\n",
        "        modality_type = getattr(ModalityType, modality_name.upper())\n",
        "        #lora_layer_idxs[modality_type] = getattr(args, f'lora_layer_idxs_{modality_name}', None)\n",
        "        # if not lora_layer_idxs[modality_type]:\n",
        "        #     lora_layer_idxs[modality_type] = None\n",
        "        lora_layer_idxs[modality_type] = None\n",
        "        lora_modality_names.append(modality_type)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown modality name: {modality_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGld7CWhKh9v"
      },
      "outputs": [],
      "source": [
        "model = ImageBindTrain(\n",
        "                        max_epochs=max_epochs, batch_size=batch_size,\n",
        "                        num_workers=num_workers, self_contrast=self_contrast,\n",
        "                        lora=lora, lora_checkpoint_dir=lora_checkpoint_dir,\n",
        "                        lora_layer_idxs=lora_layer_idxs if lora_layer_idxs else None,\n",
        "                        lora_modality_names=lora_modality_names if lora_modality_names else None,\n",
        "                        linear_probing=linear_probing\n",
        "                      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "16oYl95bUKwZ"
      },
      "outputs": [],
      "source": [
        "if full_model_checkpointing:\n",
        "        checkpointing = {\"enable_checkpointing\": full_model_checkpointing,\n",
        "                         \"callbacks\": [ModelCheckpoint(monitor=\"val_loss\", dirpath=full_model_checkpoint_dir,\n",
        "                                                        filename=\"imagebind-{epoch:02d}-{val_loss:.2f}\",\n",
        "                                                        save_last=True, mode=\"min\")]}\n",
        "else:\n",
        "        checkpointing = {\"enable_checkpointing\": full_model_checkpointing,}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "hxiqGKAST8Pr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415,
          "referenced_widgets": [
            "20b13e7dc0e540d89ce30e543859c9c6",
            "44226fd730854502b7da65261326b137",
            "6e7f4140e6154d69a03dc1846e56be1b",
            "6b009a9e6d474243b7201d83399f138d",
            "01587abfb7b647b0ba765ebde0fc2434",
            "fe249ab196644f0892d5fe245b35ac59",
            "c8008e11428240688c87adcb4606e16d",
            "b1ffe1329d0e4f07931e9c08cf65b8a9",
            "7c54405d2af7454c9d8387412ec57a90",
            "c66fbf3ff8b94c079285a5cb681d4833",
            "64f023c256784518b5ec05d6b1a8ee1e"
          ]
        },
        "outputId": "5afcdbdc-75ac-4711-c88b-ebf35356e43d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA L4') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:numexpr.utils:NumExpr defaulting to 12 threads.\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name  | Type           | Params\n",
            "-----------------------------------------\n",
            "0 | model | ImageBindModel | 1.2 B \n",
            "-----------------------------------------\n",
            "5.5 M     Trainable params\n",
            "1.2 B     Non-trainable params\n",
            "1.2 B     Total params\n",
            "4,805.516 Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20b13e7dc0e540d89ce30e543859c9c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(accelerator=\"gpu\" if \"cuda\" in device_name else \"cpu\",\n",
        "                      devices=1 if \":\" not in device_name else [int(device_name.split(\":\")[1])], deterministic=True,\n",
        "                      max_epochs=max_epochs, gradient_clip_val=gradient_clip_val,\n",
        "                      logger=loggers if loggers else None, **checkpointing)\n",
        "\n",
        "trainer.fit(model, train_loader, val_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "20b13e7dc0e540d89ce30e543859c9c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44226fd730854502b7da65261326b137",
              "IPY_MODEL_6e7f4140e6154d69a03dc1846e56be1b",
              "IPY_MODEL_6b009a9e6d474243b7201d83399f138d"
            ],
            "layout": "IPY_MODEL_01587abfb7b647b0ba765ebde0fc2434"
          }
        },
        "44226fd730854502b7da65261326b137": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe249ab196644f0892d5fe245b35ac59",
            "placeholder": "​",
            "style": "IPY_MODEL_c8008e11428240688c87adcb4606e16d",
            "value": "Sanity Checking: "
          }
        },
        "6e7f4140e6154d69a03dc1846e56be1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1ffe1329d0e4f07931e9c08cf65b8a9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c54405d2af7454c9d8387412ec57a90",
            "value": 0
          }
        },
        "6b009a9e6d474243b7201d83399f138d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c66fbf3ff8b94c079285a5cb681d4833",
            "placeholder": "​",
            "style": "IPY_MODEL_64f023c256784518b5ec05d6b1a8ee1e",
            "value": " 0/? [00:00&lt;?, ?it/s]"
          }
        },
        "01587abfb7b647b0ba765ebde0fc2434": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "fe249ab196644f0892d5fe245b35ac59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8008e11428240688c87adcb4606e16d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1ffe1329d0e4f07931e9c08cf65b8a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c54405d2af7454c9d8387412ec57a90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c66fbf3ff8b94c079285a5cb681d4833": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64f023c256784518b5ec05d6b1a8ee1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}