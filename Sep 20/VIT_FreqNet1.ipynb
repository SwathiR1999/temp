{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["root_dir =\"/content/drive/MyDrive/Zero_Shot_DeepFake_Image_Classification/\""],"metadata":{"id":"uHIMLUlQxn_s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torchvision.models import vit_b_16\n","import torch.optim as optim\n","from tqdm import tqdm\n","import os\n","from sklearn.metrics import confusion_matrix\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import random\n","import torch.fft as fft"],"metadata":{"id":"_YdDdtcSxkZw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seed = 43\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"],"metadata":{"id":"6Va7pDdFVtYf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UKETrgi0xekO"},"outputs":[],"source":["dataset_root_dir = root_dir + 'DeepfakeEmpiricalStudy/dataset/'\n","temp_dataset_root_dir = root_dir + 'dataset_small/'\n","train_dir = temp_dataset_root_dir + 'CELEB/train'\n","val_dir = temp_dataset_root_dir + 'CELEB/val'\n","test_dirs = [dataset_root_dir + 'CELEB-M/test', dataset_root_dir + 'DF/test', dataset_root_dir + 'DFD/test', \\\n","             dataset_root_dir + 'F2F/test', dataset_root_dir + 'FS-I/test', dataset_root_dir + 'NT-I/test' ]\n","\n","models_root_dir = root_dir + 'DeepfakeEmpiricalStudy_Models/'"]},{"cell_type":"code","source":["batch_size = 64\n","num_epochs = 5\n","learning_rate = 1e-4\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n","val_dataset = datasets.ImageFolder(val_dir, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","# class TransformerBasedModel(nn.Module):\n","#     def __init__(self, num_classes=2):\n","#         super(TransformerBasedModel, self).__init__()\n","#         self.vit = vit_b_16(pretrained=True)\n","#         #self.vit.heads = nn.Linear(self.vit.heads.in_features, num_classes)\n","\n","#     def forward(self, x):\n","#         return self.vit(x)"],"metadata":{"id":"9ypW6p62VUBK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FreqNetViT(nn.Module):\n","    def __init__(self, num_classes=2, patch_size=16, im_width=224, im_height=224):\n","        super(FreqNetViT, self).__init__()\n","\n","        # Load pre-trained Vision Transformer (ViT) model\n","        self.vit = vit_b_16(pretrained=True)\n","        self.vit.heads = nn.Linear(self.vit.heads.head.in_features, num_classes)  # Update the final layer\n","\n","        # Patch size (used to define high-pass filter size)\n","        self.patch_size = patch_size\n","        self.im_width = im_width\n","        self.im_height = im_height\n","\n","        self.high_pass_filter1 = self.create_high_pass_filter(self.im_width)\n","        # High-pass filter for extracting high-frequency information\n","        self.high_pass_filter = self.create_high_pass_filter(self.patch_size)\n","\n","        # Frequency convolutional layers for amplitude and phase\n","        self.freq_conv_amp = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, padding=1)\n","        self.freq_conv_phase = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, padding=1)\n","\n","    def forward(self, x):\n","        # Step 1: Convert input images to the frequency domain and apply high-pass filter\n","        # x.shape:  torch.Size([1, 3, 224, 224])\n","        x_freq = self.apply_fft_highpass(x)\n","        # x_freq.shape: torch.Size([1, 3, 224, 224])\n","\n","        # Step 2: Apply frequency convolution to the high-frequency components\n","        x_freq_convolved = self.frequency_convolution(x_freq)\n","\n","        # Step 3: Pass the frequency-transformed images to the Vision Transformer (ViT)\n","        x_vit = self.vit(x_freq_convolved)\n","\n","        return x_vit\n","\n","    def apply_fft_highpass(self, x):\n","        \"\"\"\n","        Convert image to frequency domain, apply high-pass filter, and convert back.\n","        \"\"\"\n","        # x.shape is [1, 3, 224, 224]\n","\n","        # FFT: Transform the input images to the frequency domain\n","        x_fft = fft.fftn(x, dim=(-2, -1))  # Apply FFT over spatial dimensions (height, width)\n","        # x_fft.shape: torch.Size([1, 3, 224, 224])\n","\n","\n","        # Shift zero frequency to the center\n","        x_fft_shift = fft.fftshift(x_fft)\n","        # x_fft_shift.shape: torch.Size([1, 3, 224, 224])\n","\n","        # self.high_pass_filter1.to(x.device).shape: torch.Size([224, 224])\n","\n","        # Apply high-pass filter to remove low-frequency components\n","        x_fft_high = x_fft_shift * self.high_pass_filter1.to(x.device)\n","        # x_fft_high.shape: torch.Size([1, 3, 224, 224])\n","\n","        # Inverse FFT: Convert back to the spatial domain\n","        x_fft_high_shifted = fft.ifftshift(x_fft_high)  # Shift frequencies back\n","        # x_fft_high_shifted.shape:  torch.Size([1, 3, 224, 224])\n","        x_ifft = torch.real(fft.ifftn(x_fft_high_shifted, dim=(-2, -1)))  # Inverse FFT\n","        # x_ifft.shape: torch.Size([1, 3, 224, 224])\n","\n","        return x_ifft\n","\n","    def create_high_pass_filter(self, patch_size):\n","        \"\"\"\n","        Create a high-pass filter to extract high-frequency components from patches.\n","        \"\"\"\n","        # Initialize filter to ones (no filtering)\n","        filter = torch.ones(patch_size, patch_size)\n","\n","        # Set a central region to zero (to remove low frequencies)\n","        center_x, center_y = patch_size // 2, patch_size // 2\n","        filter[center_x - patch_size//4 : center_x + patch_size//4,\n","               center_y - patch_size//4 : center_y + patch_size//4] = 0\n","\n","        return filter\n","\n","    def frequency_convolution(self, x):\n","        \"\"\"\n","        Apply convolutional layers in the frequency domain on amplitude and phase spectra.\n","        \"\"\"\n","        # FFT: Convert feature maps to the frequency domain\n","        x_fft = fft.fftn(x, dim=(-2, -1))  # FFT on spatial dimensions (height, width)\n","\n","        # Separate amplitude and phase\n","        amp = torch.abs(x_fft)  # Amplitude spectrum\n","        phase = torch.angle(x_fft)  # Phase spectrum\n","\n","        # Apply convolutions in the frequency space\n","        amp_conv = self.freq_conv_amp(amp)  # Convolution on amplitude\n","        phase_conv = self.freq_conv_phase(phase)  # Convolution on phase\n","\n","        # Reconstruct the feature maps using the modified amplitude and phase\n","        x_fft_new = torch.polar(amp_conv, phase_conv)\n","\n","        # Inverse FFT: Convert back to spatial domain\n","        x_ifft = torch.real(fft.ifftn(x_fft_new, dim=(-2, -1)))\n","        # x_ifft.shape:  torch.Size([1, 3, 224, 224])\n","\n","        return x_ifft\n","\n","\n","model = FreqNetViT(num_classes=2).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MK0V69DEValn","executionInfo":{"status":"ok","timestamp":1726744854171,"user_tz":-330,"elapsed":1892,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}},"outputId":"93e837ae-c034-402f-e3ce-ad1ae63a49b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"code","source":["def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n","    model.train()\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","        for inputs, labels in tqdm(train_loader):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * inputs.size(0)\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        train_acc = correct / total\n","        val_acc = evaluate_model(model, val_loader, criterion)[0]\n","\n","        print(\"Epoch \"+str(epoch+1)+\", Loss: \"+str(running_loss/total)+\", Train Accuracy: \"+str(train_acc)+\", Val Accuracy: \"+str(val_acc))\n","\n","        #print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/total:.4f}, Train Accuracy: {train_acc:.4f}, Val Accuracy: {val_acc:.4f}\")\n","\n","        if val_acc > best_acc:\n","            best_acc = val_acc\n","            torch.save(model.state_dict(), models_root_dir + 'best_vit_model.pth')\n","            print('Model saved!')\n","\n","    print(f\"Training complete. Best validation accuracy: {best_acc:.4f}\")\n","\n","def evaluate_model(model, loader, criterion):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    all_preds = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for inputs, labels in loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    return correct / total, np.array(all_labels), np.array(all_preds)"],"metadata":{"id":"aQ4SwYmNVfYP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_confusion_matrix(cm, classes, title='Confusion Matrix'):\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n","    plt.title(title)\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","    plt.show()"],"metadata":{"id":"XRrhtu9642ln"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7lSeHcxNACd9","executionInfo":{"status":"ok","timestamp":1726745163834,"user_tz":-330,"elapsed":309669,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}},"outputId":"a291870e-f943-4ad0-a83f-1bef1a5dab82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 8/8 [03:09<00:00, 23.74s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 0.8520173964500427, Train Accuracy: 0.528, Val Accuracy: 0.5\n","Model saved!\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 8/8 [00:11<00:00,  1.42s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2, Loss: 0.7063859400749206, Train Accuracy: 0.524, Val Accuracy: 0.5\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 8/8 [00:10<00:00,  1.37s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3, Loss: 0.7067483739852906, Train Accuracy: 0.472, Val Accuracy: 0.5\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 8/8 [00:10<00:00,  1.36s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4, Loss: 0.6962648062705994, Train Accuracy: 0.468, Val Accuracy: 0.5\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 8/8 [00:10<00:00,  1.37s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5, Loss: 0.6913810930252076, Train Accuracy: 0.514, Val Accuracy: 0.53\n","Model saved!\n","Training complete. Best validation accuracy: 0.5300\n"]}]},{"cell_type":"code","source":["model.load_state_dict(torch.load(models_root_dir + 'best_vit_model_freqnet1.pth'))\n","\n","all_labels_combined = []\n","all_preds_combined = []\n","\n","for test_dir in test_dirs:\n","    test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    test_acc, all_labels, all_preds = evaluate_model(model, test_loader, criterion)\n","    print(f\"Test Accuracy for {test_dir}: {test_acc:.4f}\")\n","\n","    cm = confusion_matrix(all_labels, all_preds, labels=[0, 1])\n","    plot_confusion_matrix(cm, classes=['real', 'fake'], title=f'Confusion Matrix for {test_dir}')\n","\n","    all_labels_combined.extend(all_labels)\n","    all_preds_combined.extend(all_preds)\n","\n","cm_combined = confusion_matrix(all_labels_combined, all_preds_combined, labels=[0, 1])\n","print(f\"Average Accuracy: {np.mean([evaluate_model(model, DataLoader(datasets.ImageFolder(test_dir, transform=transform), batch_size=batch_size, shuffle=False), criterion)[0] for test_dir in test_dirs]):.4f}\")\n","plot_confusion_matrix(cm_combined, classes=['real', 'fake'], title='Combined Confusion Matrix')"],"metadata":{"id":"Q-Rd9sd64sIB","executionInfo":{"status":"error","timestamp":1726745816017,"user_tz":-330,"elapsed":652193,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}},"colab":{"base_uri":"https://localhost:8080/","height":427},"outputId":"19e3e39e-cebf-4146-e30b-daf48a1ba049"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-10-a040d0c993e0>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(models_root_dir + 'best_vit_model.pth'))\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-a040d0c993e0>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Accuracy for {test_dir}: {test_acc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-681af8c01e8c>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, loader, criterion)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \"\"\"\n\u001b[1;32m    244\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3440\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3442\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3444\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}