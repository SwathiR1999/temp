{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMtjGkZ7Cot75qUX3Fj5QEC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.fft as fft\n","\n","class SelfAttention(nn.Module):\n","    def __init__(self, embed_size, heads):\n","        super(SelfAttention, self).__init__()\n","        self.embed_size = embed_size\n","        self.heads = heads\n","        self.head_dim = embed_size // heads\n","\n","        assert (\n","            self.head_dim * heads == embed_size\n","        ), \"Embedding size needs to be divisible by heads\"\n","\n","        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n","\n","    def forward(self, values, keys, query, mask):\n","        N = query.shape[0]\n","        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n","\n","        # Split embedding into self.heads different pieces\n","        values = values.reshape(N, value_len, self.heads, self.head_dim)\n","        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n","        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n","\n","        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n","        if mask is not None:\n","            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n","\n","        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n","\n","        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n","            N, query_len, self.heads * self.head_dim\n","        )\n","\n","        return self.fc_out(out)\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, embed_size, heads, dropout, forward_expansion):\n","        super(TransformerBlock, self).__init__()\n","        self.attention = SelfAttention(embed_size, heads)\n","        self.norm1 = nn.LayerNorm(embed_size)\n","        self.norm2 = nn.LayerNorm(embed_size)\n","\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(embed_size, forward_expansion * embed_size),\n","            nn.ReLU(),\n","            nn.Linear(forward_expansion * embed_size, embed_size)\n","        )\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, value, key, query, mask):\n","        attention = self.attention(value, key, query, mask)\n","\n","        x = self.dropout(self.norm1(attention + query))\n","        forward = self.feed_forward(x)\n","        out = self.dropout(self.norm2(forward + x))\n","        return out\n","\n","class FreqNetSimpleTransformer(nn.Module):\n","    def __init__(self, num_classes=2, patch_size=16, im_width=224, im_height=224, d_model=768, num_heads=8, num_layers=6, dropout=0.1):\n","        super(FreqNetSimpleTransformer, self).__init__()\n","\n","        # Parameters\n","        self.patch_size = patch_size\n","        self.im_width = im_width\n","        self.im_height = im_height\n","\n","        # Create high-pass filter\n","        self.high_pass_filter = self.create_high_pass_filter(self.patch_size)\n","        self.high_pass_filter1 = self.create_high_pass_filter(self.im_width)\n","\n","        # Frequency convolutional layers for amplitude and phase\n","        self.freq_conv_amp = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, padding=1)\n","        self.freq_conv_phase = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, padding=1)\n","\n","        # Positional embedding and patch embedding\n","        self.pos_embedding = nn.Parameter(torch.randn((im_width // patch_size) ** 2 + 1, 1, d_model))\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n","        self.patch_to_embedding = nn.Linear(patch_size * patch_size * 3, d_model)\n","\n","        # Transformer layers\n","        self.transformer_blocks = nn.ModuleList(\n","            [TransformerBlock(d_model, num_heads, dropout=dropout, forward_expansion=4) for _ in range(num_layers)]\n","        )\n","\n","        # Final classification head\n","        self.fc = nn.Linear(d_model, num_classes)\n","\n","    def forward(self, x):\n","\n","        # x.shape:  torch.Size([1, 3, 224, 224])\n","        # Step 1: Convert input images to the frequency domain and apply high-pass filter\n","        x_freq = self.apply_fft_highpass(x)\n","\n","        # Step 2: Apply frequency convolution to the high-frequency components\n","        x_freq_convolved = self.frequency_convolution(x_freq)\n","        # x_freq_convolved.shape:  torch.Size([1, 3, 224, 224])\n","\n","        # Step 3: Convert the image into patches and embed\n","        x_patches = self.create_patches(x_freq_convolved)\n","        # x_patches.shape:  torch.Size([196, 1, 768]) # here 1 is the batch size...if batch size is 2 , then x_patches.shape: torch.Size([196, 2, 768])\n","\n","\n","        # Step 4: Add positional encoding\n","        n_patches,batch_size,   _ = x_patches.shape # n_patches = 196, batch_size = 1\n","        # self.cls_token.shape: torch.Size([1, 1, 768])...when batch size is 2, then also self.cls_token.shape: torch.Size([1, 1, 768])\n","        cls_tokens = self.cls_token.expand(-1, batch_size, -1) #to repeat the class token for each batch.\n","\n","        # cls_tokens.shape: torch.Size([1, 1, 768]) ...if batch size was 2, then cls_tokens.shape: torch.Size([1, 2, 768])\n","        x_patches = torch.cat((cls_tokens, x_patches), dim=0)\n","        # x_patches.shape: torch.Size([197, 1, 768])... if batch size is 2, then x_patches.shape: torch.Size([197, 2, 768])\n","        # self.pos_embedding.shape: torch.Size([197, 1, 768])...if batch size is 2 then also self.pos_embedding.shape: torch.Size([197, 1, 768])\n","\n","        ###################################################\n","        # if batch size is 2\n","        # x_patches.shape: torch.Size([197, 2, 768])\n","        # self.pos_embedding.shape: torch.Size([197, 1, 768])\n","        ###################################################\n","\n","        # self.pos_embedding[:n_patches + 1,:, :].shape: torch.Size([197, 1, 768])...if batch size is 2, then also self.pos_embedding[:n_patches + 1,:, :].shape: torch.Size([197, 1, 768])\n","        x_patches += self.pos_embedding[:n_patches + 1,:, :]\n","        # x_patches.shape: torch.Size([197, 1, 768])...if batch size is 2, then x_patches.shape: torch.Size([197, 2, 768])\n","\n","\n","        # Step 5: Pass through transformer layers\n","        for transformer_block in self.transformer_blocks:\n","            x_patches = transformer_block(x_patches, x_patches, x_patches, mask=None)\n","\n","        # x_patches.shape: torch.Size([197, 1, 768]) ... if batch_size is 2 then x_patches.shape: torch.Size([197, 2, 768])\n","\n","\n","        # Step 6: Classification using the cls_token output\n","        out = self.fc(x_patches[0])\n","\n","        return out\n","\n","    def create_patches(self, x):\n","        \"\"\"\n","        Convert input images to patches and flatten them for transformer input.\n","        \"\"\"\n","        batch_size, channels, height, width = x.shape\n","        patches = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n","        # patches.shape: torch.Size([1, 3, 14, 14, 16, 16])\n","        patches = patches.contiguous().view(batch_size, channels, -1, self.patch_size * self.patch_size)  # Flatten patches\n","        # patches.shape: torch.Size([1, 3, 196, 256])\n","        patches = patches.permute(2, 0, 1, 3).contiguous().view(-1, batch_size, self.patch_size * self.patch_size * channels)  # Rearrange for transformer\n","        # patches.shape :  torch.Size([196, 1, 768])\n","        patches = self.patch_to_embedding(patches)\n","        # patches.shape: torch.Size([196, 1, 768])\n","        return patches\n","\n","    def apply_fft_highpass(self, x):\n","        \"\"\"\n","        Convert image to frequency domain, apply high-pass filter, and convert back.\n","        \"\"\"\n","        x_fft = fft.fftn(x, dim=(-2, -1))  # Apply FFT over spatial dimensions (height, width)\n","        x_fft_shift = fft.fftshift(x_fft)  # Shift zero frequency to the center\n","\n","        # Apply high-pass filter to remove low-frequency components\n","        x_fft_high = x_fft_shift * self.high_pass_filter1.to(x.device)\n","\n","        # Inverse FFT: Convert back to the spatial domain\n","        x_fft_high_shifted = fft.ifftshift(x_fft_high)  # Shift frequencies back\n","        x_ifft = torch.real(fft.ifftn(x_fft_high_shifted, dim=(-2, -1)))  # Inverse FFT\n","\n","        return x_ifft\n","\n","    def create_high_pass_filter(self, patch_size):\n","        \"\"\"\n","        Create a high-pass filter to extract high-frequency components from patches.\n","        \"\"\"\n","        filter = torch.ones(patch_size, patch_size)\n","        center_x, center_y = patch_size // 2, patch_size // 2\n","        filter[center_x - patch_size // 4:center_x + patch_size // 4,\n","               center_y - patch_size // 4:center_y + patch_size // 4] = 0  # Zero central region to keep high-frequencies\n","        return filter\n","\n","    def frequency_convolution(self, x):\n","        \"\"\"\n","        Apply convolutional layers in the frequency domain on amplitude and phase spectra.\n","        \"\"\"\n","        x_fft = fft.fftn(x, dim=(-2, -1))  # FFT on spatial dimensions (height, width)\n","\n","        amp = torch.abs(x_fft)  # Amplitude spectrum\n","        phase = torch.angle(x_fft)  # Phase spectrum\n","\n","        # Apply convolutions in the frequency space\n","        amp_conv = self.freq_conv_amp(amp)  # Convolution on amplitude\n","        phase_conv = self.freq_conv_phase(phase)  # Convolution on phase\n","\n","        # Reconstruct the feature maps using the modified amplitude and phase\n","        x_fft_new = torch.polar(amp_conv, phase_conv)\n","\n","        # Inverse FFT: Convert back to spatial domain\n","        x_ifft = torch.real(fft.ifftn(x_fft_new, dim=(-2, -1)))\n","\n","        return x_ifft\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    # Example input tensor: batch of images with 3 channels, 224x224 dimensions\n","    input_tensor = torch.randn(1, 3, 224, 224)\n","\n","    # Initialize model\n","    model = FreqNetSimpleTransformer(num_classes=2, patch_size=16, im_width=224, im_height=224, d_model=768, num_heads=8, num_layers=6, dropout=0.1)\n","\n","    # Forward pass\n","    output = model(input_tensor)\n","\n","    # Output shape\n","    print(\"Output shape:\", output.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LjokIBwNhzul","executionInfo":{"status":"ok","timestamp":1726764853478,"user_tz":-330,"elapsed":1138,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}},"outputId":"6af2dcd8-0304-4d0e-aff6-483d6cae00f9"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["Output shape: torch.Size([1, 2])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"DwABApdCmhpe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["patch_size = 16\n","\n","x = torch.randn(1, 3, 224, 224)\n","h = x.unfold(2, patch_size, patch_size)"],"metadata":{"id":"NyyDw0CYmjE5","executionInfo":{"status":"ok","timestamp":1726754780032,"user_tz":-330,"elapsed":396,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["h.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nbeXAecOmvcD","executionInfo":{"status":"ok","timestamp":1726754793135,"user_tz":-330,"elapsed":360,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}},"outputId":"df4900cd-92a5-4770-89a1-87cc0a97f197"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 3, 14, 224, 16])"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["import torch\n","\n","# Smaller tensor for demonstration\n","x = torch.arange(1, 1 + 1 * 3 * 8 * 8).view(1, 3, 8, 8)\n","patch_size = 4\n","\n","# Apply unfold to simulate patch extraction\n","h = x.unfold(2, patch_size, patch_size)\n","\n","# Prepare to display results\n","x_values = x.numpy()\n","h_values = h.numpy()\n","\n","x_values, h_values"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oh4F43G8rDex","executionInfo":{"status":"ok","timestamp":1726755934832,"user_tz":-330,"elapsed":368,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}},"outputId":"7cd965dc-180f-4ec5-9469-2c241c91256a"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[[[  1,   2,   3,   4,   5,   6,   7,   8],\n","          [  9,  10,  11,  12,  13,  14,  15,  16],\n","          [ 17,  18,  19,  20,  21,  22,  23,  24],\n","          [ 25,  26,  27,  28,  29,  30,  31,  32],\n","          [ 33,  34,  35,  36,  37,  38,  39,  40],\n","          [ 41,  42,  43,  44,  45,  46,  47,  48],\n","          [ 49,  50,  51,  52,  53,  54,  55,  56],\n","          [ 57,  58,  59,  60,  61,  62,  63,  64]],\n"," \n","         [[ 65,  66,  67,  68,  69,  70,  71,  72],\n","          [ 73,  74,  75,  76,  77,  78,  79,  80],\n","          [ 81,  82,  83,  84,  85,  86,  87,  88],\n","          [ 89,  90,  91,  92,  93,  94,  95,  96],\n","          [ 97,  98,  99, 100, 101, 102, 103, 104],\n","          [105, 106, 107, 108, 109, 110, 111, 112],\n","          [113, 114, 115, 116, 117, 118, 119, 120],\n","          [121, 122, 123, 124, 125, 126, 127, 128]],\n"," \n","         [[129, 130, 131, 132, 133, 134, 135, 136],\n","          [137, 138, 139, 140, 141, 142, 143, 144],\n","          [145, 146, 147, 148, 149, 150, 151, 152],\n","          [153, 154, 155, 156, 157, 158, 159, 160],\n","          [161, 162, 163, 164, 165, 166, 167, 168],\n","          [169, 170, 171, 172, 173, 174, 175, 176],\n","          [177, 178, 179, 180, 181, 182, 183, 184],\n","          [185, 186, 187, 188, 189, 190, 191, 192]]]]),\n"," array([[[[[  1,   9,  17,  25],\n","           [  2,  10,  18,  26],\n","           [  3,  11,  19,  27],\n","           [  4,  12,  20,  28],\n","           [  5,  13,  21,  29],\n","           [  6,  14,  22,  30],\n","           [  7,  15,  23,  31],\n","           [  8,  16,  24,  32]],\n"," \n","          [[ 33,  41,  49,  57],\n","           [ 34,  42,  50,  58],\n","           [ 35,  43,  51,  59],\n","           [ 36,  44,  52,  60],\n","           [ 37,  45,  53,  61],\n","           [ 38,  46,  54,  62],\n","           [ 39,  47,  55,  63],\n","           [ 40,  48,  56,  64]]],\n"," \n"," \n","         [[[ 65,  73,  81,  89],\n","           [ 66,  74,  82,  90],\n","           [ 67,  75,  83,  91],\n","           [ 68,  76,  84,  92],\n","           [ 69,  77,  85,  93],\n","           [ 70,  78,  86,  94],\n","           [ 71,  79,  87,  95],\n","           [ 72,  80,  88,  96]],\n"," \n","          [[ 97, 105, 113, 121],\n","           [ 98, 106, 114, 122],\n","           [ 99, 107, 115, 123],\n","           [100, 108, 116, 124],\n","           [101, 109, 117, 125],\n","           [102, 110, 118, 126],\n","           [103, 111, 119, 127],\n","           [104, 112, 120, 128]]],\n"," \n"," \n","         [[[129, 137, 145, 153],\n","           [130, 138, 146, 154],\n","           [131, 139, 147, 155],\n","           [132, 140, 148, 156],\n","           [133, 141, 149, 157],\n","           [134, 142, 150, 158],\n","           [135, 143, 151, 159],\n","           [136, 144, 152, 160]],\n"," \n","          [[161, 169, 177, 185],\n","           [162, 170, 178, 186],\n","           [163, 171, 179, 187],\n","           [164, 172, 180, 188],\n","           [165, 173, 181, 189],\n","           [166, 174, 182, 190],\n","           [167, 175, 183, 191],\n","           [168, 176, 184, 192]]]]]))"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":[],"metadata":{"id":"S8llkXyDIq80"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create integer tensor 'a' of shape (2, 3, 4)\n","a = torch.randint(0, 10, (2, 3, 4), dtype=torch.int)\n","\n","# Create integer tensor 'b' of shape (2, 1, 4)\n","b = torch.randint(0, 10, (2, 1, 4), dtype=torch.int)"],"metadata":{"id":"QEBRG_jJIpX5","executionInfo":{"status":"ok","timestamp":1726764172815,"user_tz":-330,"elapsed":518,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["a"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aY4jZhtcJWzZ","executionInfo":{"status":"ok","timestamp":1726764178058,"user_tz":-330,"elapsed":1643,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}},"outputId":"f5373f38-f00d-4607-f77b-6243f7f2361a"},"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0, 7, 1, 3],\n","         [5, 9, 7, 4],\n","         [3, 4, 2, 8]],\n","\n","        [[5, 6, 9, 4],\n","         [8, 0, 3, 9],\n","         [2, 7, 0, 2]]], dtype=torch.int32)"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["b"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9L0xhFHfJXhD","executionInfo":{"status":"ok","timestamp":1726764178058,"user_tz":-330,"elapsed":12,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}},"outputId":"818a0431-9e58-4ec7-ff24-993b2c11b7aa"},"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[2, 9, 0, 2]],\n","\n","        [[7, 0, 0, 8]]], dtype=torch.int32)"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","source":["a+b"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E-Ihtw7GJJTH","executionInfo":{"status":"ok","timestamp":1726764178058,"user_tz":-330,"elapsed":10,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}},"outputId":"bce8b17f-43bb-4b5f-d5f7-e47017b247c4"},"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 2, 16,  1,  5],\n","         [ 7, 18,  7,  6],\n","         [ 5, 13,  2, 10]],\n","\n","        [[12,  6,  9, 12],\n","         [15,  0,  3, 17],\n","         [ 9,  7,  0, 10]]], dtype=torch.int32)"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","source":["(a+b).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9k6ZiXkeJO5D","executionInfo":{"status":"ok","timestamp":1726764178059,"user_tz":-330,"elapsed":9,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}},"outputId":"277160f7-77b6-40f3-c573-c237119f47d4"},"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 3, 4])"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["a[:2,:,:].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65BetAH5I0sP","executionInfo":{"status":"ok","timestamp":1726764178059,"user_tz":-330,"elapsed":7,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}},"outputId":"0b76cc25-08f8-4fb1-bb6b-2cbdee524773"},"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 3, 4])"]},"metadata":{},"execution_count":63}]}]}