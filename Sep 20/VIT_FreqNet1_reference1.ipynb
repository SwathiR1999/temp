{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.fft as fft\n","from torchvision.models import vit_b_16\n","\n","class FreqNetViT(nn.Module):\n","    def __init__(self, num_classes=2, patch_size=16, im_width=224, im_height=224):\n","        super(FreqNetViT, self).__init__()\n","\n","        # Load pre-trained Vision Transformer (ViT) model\n","        self.vit = vit_b_16(pretrained=True)\n","        self.vit.heads = nn.Linear(self.vit.heads.head.in_features, num_classes)  # Update the final layer\n","\n","        # Patch size (used to define high-pass filter size)\n","        self.patch_size = patch_size\n","        self.im_width = im_width\n","        self.im_height = im_height\n","\n","        self.high_pass_filter1 = self.create_high_pass_filter(self.im_width)\n","        # High-pass filter for extracting high-frequency information\n","        self.high_pass_filter = self.create_high_pass_filter(self.patch_size)\n","\n","        # Frequency convolutional layers for amplitude and phase\n","        self.freq_conv_amp = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, padding=1)\n","        self.freq_conv_phase = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, padding=1)\n","\n","    def forward(self, x):\n","        # Step 1: Convert input images to the frequency domain and apply high-pass filter\n","        # x.shape:  torch.Size([1, 3, 224, 224])\n","        x_freq = self.apply_fft_highpass(x)\n","        # x_freq.shape: torch.Size([1, 3, 224, 224])\n","\n","        # Step 2: Apply frequency convolution to the high-frequency components\n","        x_freq_convolved = self.frequency_convolution(x_freq)\n","\n","        # Step 3: Pass the frequency-transformed images to the Vision Transformer (ViT)\n","        x_vit = self.vit(x_freq_convolved)\n","\n","        return x_vit\n","\n","    def apply_fft_highpass(self, x):\n","        \"\"\"\n","        Convert image to frequency domain, apply high-pass filter, and convert back.\n","        \"\"\"\n","        # x.shape is [1, 3, 224, 224]\n","\n","        # FFT: Transform the input images to the frequency domain\n","        x_fft = fft.fftn(x, dim=(-2, -1))  # Apply FFT over spatial dimensions (height, width)\n","        # x_fft.shape: torch.Size([1, 3, 224, 224])\n","\n","\n","        # Shift zero frequency to the center\n","        x_fft_shift = fft.fftshift(x_fft)\n","        # x_fft_shift.shape: torch.Size([1, 3, 224, 224])\n","\n","        # self.high_pass_filter1.to(x.device).shape: torch.Size([224, 224])\n","\n","        # Apply high-pass filter to remove low-frequency components\n","        x_fft_high = x_fft_shift * self.high_pass_filter1.to(x.device)\n","        # x_fft_high.shape: torch.Size([1, 3, 224, 224])\n","\n","        # Inverse FFT: Convert back to the spatial domain\n","        x_fft_high_shifted = fft.ifftshift(x_fft_high)  # Shift frequencies back\n","        # x_fft_high_shifted.shape:  torch.Size([1, 3, 224, 224])\n","        x_ifft = torch.real(fft.ifftn(x_fft_high_shifted, dim=(-2, -1)))  # Inverse FFT\n","        # x_ifft.shape: torch.Size([1, 3, 224, 224])\n","\n","        return x_ifft\n","\n","    def create_high_pass_filter(self, patch_size):\n","        \"\"\"\n","        Create a high-pass filter to extract high-frequency components from patches.\n","        \"\"\"\n","        # Initialize filter to ones (no filtering)\n","        filter = torch.ones(patch_size, patch_size)\n","\n","        # Set a central region to zero (to remove low frequencies)\n","        center_x, center_y = patch_size // 2, patch_size // 2\n","        filter[center_x - patch_size//4 : center_x + patch_size//4,\n","               center_y - patch_size//4 : center_y + patch_size//4] = 0\n","\n","        return filter\n","\n","    def frequency_convolution(self, x):\n","        \"\"\"\n","        Apply convolutional layers in the frequency domain on amplitude and phase spectra.\n","        \"\"\"\n","        # FFT: Convert feature maps to the frequency domain\n","        x_fft = fft.fftn(x, dim=(-2, -1))  # FFT on spatial dimensions (height, width)\n","\n","        # Separate amplitude and phase\n","        amp = torch.abs(x_fft)  # Amplitude spectrum\n","        phase = torch.angle(x_fft)  # Phase spectrum\n","\n","        # Apply convolutions in the frequency space\n","        amp_conv = self.freq_conv_amp(amp)  # Convolution on amplitude\n","        phase_conv = self.freq_conv_phase(phase)  # Convolution on phase\n","\n","        # Reconstruct the feature maps using the modified amplitude and phase\n","        x_fft_new = torch.polar(amp_conv, phase_conv)\n","\n","        # Inverse FFT: Convert back to spatial domain\n","        x_ifft = torch.real(fft.ifftn(x_fft_new, dim=(-2, -1)))\n","        # x_ifft.shape:  torch.Size([1, 3, 224, 224])\n","\n","        return x_ifft\n","\n","# Instantiate the model\n","model = FreqNetViT(num_classes=2)\n","\n","# Example input: A batch of images with size (batch_size, channels, height, width)\n","input_tensor = torch.randn(1, 3, 224, 224)  # 1 image, 3 channels (RGB), 224x224 resolution\n","\n","# Forward pass\n","output = model(input_tensor)\n","\n","# Output shape\n","print(\"Output shape:\", output.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bJe0pAl3y4aq","executionInfo":{"status":"ok","timestamp":1726743019799,"user_tz":-330,"elapsed":2547,"user":{"displayName":"SWATHI R","userId":"01338665695010248846"}},"outputId":"c4104b8e-0384-4f95-e973-324e7174c4c5"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["x_ifft.shape:  torch.Size([1, 3, 224, 224])\n","Output shape: torch.Size([1, 2])\n"]}]}]}