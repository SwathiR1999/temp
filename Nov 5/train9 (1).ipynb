{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTTOw7cYsO62",
        "outputId": "5620c622-898d-48cd-ea88-acb9b875b79c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorboard --quiet"
      ],
      "metadata": {
        "id": "sGRbhNyBTPyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# # Initialize TensorBoard writer\n",
        "# writer = SummaryWriter(log_dir='runs/my_model')"
      ],
      "metadata": {
        "id": "g-leDhrZTTIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zv2N2Teaevhv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "from clip import clip\n",
        "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed=0):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "seed_everything()"
      ],
      "metadata": {
        "id": "qwah0PjLr7EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_tokenizer = _Tokenizer()"
      ],
      "metadata": {
        "id": "ltbV0qIqvHOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Attention**"
      ],
      "metadata": {
        "id": "RTn1olOs2bOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContextAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads):\n",
        "        super(ContextAttention, self).__init__()\n",
        "        self.attention_layer = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first= True)\n",
        "\n",
        "    def forward(self, image_features, ctx_features):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_features: Tensor of shape [b, 196, 512] (batch size, seq length, feature size)\n",
        "            ctx_features: Tensor of shape [b, n_ctx, 512] (context features for 'real' and 'fake')\n",
        "        Returns:\n",
        "            Tensor of shape [b, n_ctx, 512] containing attended context features.\n",
        "        \"\"\"\n",
        "        attn_output, attn_weight = self.attention_layer(ctx_features, image_features, image_features)\n",
        "        return attn_output, attn_weight # attn_output shape is (b, n_ctx, 512)"
      ],
      "metadata": {
        "id": "mRRho2pIfJMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PromptLearner**"
      ],
      "metadata": {
        "id": "kWLM5Hx12YkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PromptLearner(nn.Module):\n",
        "    def __init__(self, n_ctx, classnames, clip_model):\n",
        "        super().__init__()\n",
        "        dtype = clip_model.dtype\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        ctx_dim = clip_model.ln_final.weight.shape[0] # clip_model.ln_final.weight.shape[0] is 512\n",
        "        self.n_cls = len(classnames)\n",
        "\n",
        "        ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=dtype, device = device)\n",
        "\n",
        "        nn.init.normal_(ctx_vectors)\n",
        "\n",
        "        self.ctx = nn.Parameter(ctx_vectors)  # to be optimized --> shape = [n_ctx, 512]\n",
        "\n",
        "        self.ctx_attn = ContextAttention(512, 8)\n",
        "\n",
        "        prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
        "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
        "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(device) # tokenized_prompts.shape is (2, 77)\n",
        "        with torch.no_grad():\n",
        "                embedding = clip_model.token_embedding(tokenized_prompts).type(dtype) # embedding.shape = torch.Size([2, 77, 512])\n",
        "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS --> shape = [2, 1, 512]\n",
        "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS -->shape = [2, 61, 512]\n",
        "        self.tokenized_prompts = tokenized_prompts # tokenized_prompts.shape is (2, 77)\n",
        "\n",
        "        self.img_ln = nn.LayerNorm(512)\n",
        "        self.ctx_ln = nn.LayerNorm(512)\n",
        "        self.ctx_attn_ln_pre = nn.LayerNorm(ctx_dim)  # ctx_dim = 512\n",
        "        self.ctx_attn_ln_post = nn.LayerNorm(ctx_dim)\n",
        "\n",
        "\n",
        "    def forward(self, image_features, global_step=None):\n",
        "        # image_features.shape is [b, 196, 512]\n",
        "        ctx_features = self.ctx.unsqueeze(0) # shape = [1, n_ctx, 512]\n",
        "        ctx_features = ctx_features.repeat(image_features.shape[0], 1, 1) # shape = [b, n_ctx, 512]\n",
        "\n",
        "        # Standardize image_features\n",
        "        image_features_standardized = self.img_ln(image_features)\n",
        "        # writer.add_histogram('image_features_standardized', image_features_standardized, global_step)\n",
        "\n",
        "        # Standardize ctx_features\n",
        "        ctx_features_standardized = self.ctx_ln(ctx_features)\n",
        "        # writer.add_histogram('ctx_features_standardized', ctx_features_standardized, global_step)\n",
        "\n",
        "        # Patch enhanced attention calculation\n",
        "        ctx_features_attn_output, ctx_features_attn_weight = self.ctx_attn(image_features_standardized, ctx_features_standardized) # shape = [b, n_ctx, 512]\n",
        "        # writer.add_histogram('ctx_features_attn_output', ctx_features_attn_output, global_step)\n",
        "\n",
        "        # Normalizing ctx_features_attention_output\n",
        "        ctx_features_attn_output_standardized = self.ctx_attn_ln_pre(ctx_features_attn_output)\n",
        "        # writer.add_histogram('ctx_features_attn_output_standardized', ctx_features_attn_output_standardized, global_step)\n",
        "\n",
        "        ctx_features_attn_output = ctx_features_attn_output_standardized + ctx_features_standardized # shape = [b, n_ctx, 512]\n",
        "        # writer.add_histogram('ctx_features_attn_output_standardized + ctx_features_standardized', ctx_features_attn_output, global_step)\n",
        "\n",
        "        # Normalizing ctx_features_attention_output\n",
        "        ctx_features_attn_output = self.ctx_attn_ln_post(ctx_features_attn_output)\n",
        "        # writer.add_histogram('ctx_features_attn_output_final', ctx_features_attn_output, global_step)\n",
        "\n",
        "        ctx_features_attn_output = ctx_features_attn_output.mean(dim = 0) # shape = [n_ctx, 512]\n",
        "\n",
        "        ctx_features_attn_output = ctx_features_attn_output.unsqueeze(0) # shape = [1, n_ctx, 512]\n",
        "        ctx_features_attn_output = ctx_features_attn_output.expand(self.n_cls, -1, -1) #shape = [2, n_ctx, 512]\n",
        "        # writer.add_histogram('before_norm_token_prefix', self.token_prefix, global_step)\n",
        "        # writer.add_histogram('before_norm_token_suffix', self.token_suffix, global_step)\n",
        "\n",
        "        self.token_prefix = nn.functional.normalize(self.token_prefix, dim=-1)\n",
        "        self.token_suffix = nn.functional.normalize(self.token_suffix, dim=-1)\n",
        "\n",
        "        # writer.add_histogram('after_norm_token_prefix', self.token_prefix, global_step)\n",
        "        # writer.add_histogram('after_norm_token_suffix', self.token_suffix, global_step)\n",
        "\n",
        "        # writer.add_histogram('final_ctx_features_attn_output', ctx_features_attn_output, global_step)\n",
        "\n",
        "\n",
        "        prompts = torch.cat([self.token_prefix, ctx_features_attn_output, self.token_suffix], dim=1) # shape = [2, 1 + n_ctx + * , 512]\n",
        "        # shape of prompts = [2, 77, 512]\n",
        "\n",
        "        prompts_mean = prompts.mean(dim=1, keepdim=True)\n",
        "        prompts_std = prompts.std(dim=1, keepdim=True)\n",
        "        prompts = (prompts - prompts_mean) / prompts_std # shape of prompts = [2, 77, 512]\n",
        "\n",
        "        return prompts\n"
      ],
      "metadata": {
        "id": "dqoHhGAadITe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TextEncoder**"
      ],
      "metadata": {
        "id": "U3KGMBIPwE-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This TextEncoder class is designed to process text using elements from the CLIP model's architecture.\n",
        "It uses a transformer module to encode a series of tokenized text prompts and extract features from the \"end of text\" (EOT) token,\n",
        "which often represents a summary of the input sequence in CLIP's text encoding.\n",
        "'''\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, clip_model):\n",
        "        super().__init__()\n",
        "        self.transformer = clip_model.transformer\n",
        "        self.positional_embedding = clip_model.positional_embedding\n",
        "        self.ln_final = clip_model.ln_final #layer normalization module\n",
        "        self.text_projection = clip_model.text_projection # A linear projection matrix to map transformer outputs to a feature space used by the CLIP model.\n",
        "        self.dtype = clip_model.dtype\n",
        "\n",
        "    def forward(self, prompts, tokenized_prompts):\n",
        "        x = prompts + self.positional_embedding.type(self.dtype) # shape of positional_embedding = torch.Size([77, 512])\n",
        "        '''\n",
        "        x.shape = [ batch_size (N), n_ctx (L), transformer.width (D) ]\n",
        "        '''\n",
        "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
        "        x = self.ln_final(x).type(self.dtype)\n",
        "\n",
        "        # x.shape is  torch.Size([2, 77, 512])\n",
        "\n",
        "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
        "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
        "        '''\n",
        "        x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] selects the feature vector corresponding to the EOT token for each prompt in the batch.\n",
        "        This vector is then linearly projected with self.text_projection, giving the final encoded feature vector for each text prompt.\n",
        "        '''\n",
        "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
        "        # self.text_projection.shape =  torch.Size([512, 512])\n",
        "        # x.shape =  torch.Size([2, 512])\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "tQBpNZglbnh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MultiscaleAdapter**"
      ],
      "metadata": {
        "id": "TYv8IBlN3s26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiscaleAdapter(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(input_dim, input_dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(input_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(input_dim, input_dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(input_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(input_dim, input_dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(input_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Modify branches to avoid errors\n",
        "        self.branch_f = nn.Sequential(\n",
        "            nn.Conv2d(input_dim, input_dim, kernel_size=1, padding=0),\n",
        "            nn.BatchNorm2d(input_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(input_dim, input_dim, kernel_size=1, padding=0),\n",
        "            nn.BatchNorm2d(input_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.branch_g = nn.Sequential(\n",
        "            nn.Conv2d(input_dim, input_dim, kernel_size=1, padding=0),\n",
        "            nn.BatchNorm2d(input_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(input_dim, input_dim, kernel_size=3, padding=1, dilation=1),\n",
        "            nn.BatchNorm2d(input_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.branch_h = nn.Sequential(\n",
        "            nn.Conv2d(input_dim, input_dim, kernel_size=1, padding=0),\n",
        "            nn.BatchNorm2d(input_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(input_dim, input_dim, kernel_size=3, padding=1, dilation=1),\n",
        "            nn.BatchNorm2d(input_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Combine features and project back to original dimension\n",
        "        self.combine = nn.Sequential(\n",
        "            nn.Conv2d(input_dim * 3, input_dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(input_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.final_projection = nn.Sequential(\n",
        "            nn.Conv2d(input_dim, input_dim, kernel_size=1),\n",
        "            nn.BatchNorm2d(input_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape = (32, 768, 196)\n",
        "        x = x.unsqueeze(2)\n",
        "        # Now x.shape =  torch.Size([32, 768, 1, 196])\n",
        "        x = x.reshape(x.shape[0], x.shape[1], int(math.sqrt(x.shape[3])), int(math.sqrt(x.shape[3])))\n",
        "        # x.shape =  torch.Size([32, 768, 14, 14])\n",
        "        x_fg_input = self.conv1(x)\n",
        "        x_h_input = self.conv2(x)\n",
        "\n",
        "        # Branch processing\n",
        "        x_f = self.branch_f(x_fg_input)\n",
        "        x_g = self.branch_g(x_fg_input)\n",
        "        x_h = self.branch_h(x_h_input)\n",
        "\n",
        "        # Concatenating along channel dimension and projecting back\n",
        "        x_out = torch.cat([x_f, x_g, x_h], dim=1)\n",
        "        x_out = self.combine(x_out)\n",
        "\n",
        "        # Adding the processed multi-scale features to the original input\n",
        "        x_out = self.final_projection(x_out) + self.conv3(x)\n",
        "        x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], -1)\n",
        "\n",
        "        return x_out"
      ],
      "metadata": {
        "id": "otexsyJWkyN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CustomCLIP**"
      ],
      "metadata": {
        "id": "30jOsBcMvrRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCLIP(nn.Module):\n",
        "    def __init__(self, n_ctx, classnames, clip_model):\n",
        "        super().__init__()\n",
        "        self.prompt_learner = PromptLearner(n_ctx, classnames, clip_model)\n",
        "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts # tokenized_prompts.shape is (2, 77)\n",
        "\n",
        "        self.image_encoder = clip_model.visual\n",
        "        self.patch_embedding = self.image_encoder.conv1\n",
        "        self.class_embedding = self.image_encoder.class_embedding\n",
        "        self.positional_embedding = self.image_encoder.positional_embedding\n",
        "\n",
        "        for param in self.patch_embedding.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.class_embedding.requires_grad = True\n",
        "        self.positional_embedding.requires_grad = False\n",
        "\n",
        "        self.ln_pre = self.image_encoder.ln_pre\n",
        "        for param in self.image_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.text_encoder = TextEncoder(clip_model)\n",
        "        for param in self.text_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        embed_dim = self.image_encoder.ln_post.weight.shape[0]\n",
        "        self.multiscale_adapters = nn.ModuleList(\n",
        "                                    [MultiscaleAdapter(input_dim=embed_dim)\n",
        "                                    for _ in range(len(self.image_encoder.transformer.resblocks))]\n",
        "                                  )\n",
        "        self.image_features_proj = nn.Linear(embed_dim, 512)\n",
        "\n",
        "        self.logit_scale = clip_model.logit_scale\n",
        "        self.logit_scale.requires_grad = True\n",
        "\n",
        "        self.dtype = clip_model.dtype\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.4)\n",
        "\n",
        "        self.multihead_attn = nn.MultiheadAttention(embed_dim=512, num_heads=8)\n",
        "\n",
        "\n",
        "    def forward(self, images, global_step=None):\n",
        "        x = self.patch_embedding(images.type(torch.float32))  # (32, 768, 14, 14)\n",
        "        x = x.reshape(x.shape[0], x.shape[1], -1).permute(0, 2, 1)  # (batch, num_patches, embed_dim) i.e; (32, 196, 768)\n",
        "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)\n",
        "        # Now x.shape is [batch_size, num_patches + 1, 768] i.e; torch.Size([32, 197, 768])\n",
        "        # writer.add_histogram('before ln_pre', x, global_step)\n",
        "        x = x + self.positional_embedding.to(x.dtype)\n",
        "        x = self.ln_pre(x) # shape = [32, 197, 768]\n",
        "        # writer.add_histogram('after_ln_pre', x, global_step)\n",
        "        # Now x.shape is torch.Size([32, 197, 768])\n",
        "        #x = x[:,1:,:] # shape = [32, 196, 768]\n",
        "        x = self.dropout(x) #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "        for i, block in enumerate(self.image_encoder.transformer.resblocks):\n",
        "            x = block(x) # shape = [32, 197, 768]\n",
        "            # writer.add_histogram(f'after_resblock_{i}', x, global_step)\n",
        "            x_cls = x[:,0,:] # shape = [32, 768]\n",
        "            x_cls = x_cls.unsqueeze(1) # shape = [32, 1, 768]\n",
        "            x = x[:,1:,:] # shape = [32, 196, 768]\n",
        "            x = x.permute(0, 2, 1) # shape = [32, 768, 196]\n",
        "            msa = self.multiscale_adapters[i](x)\n",
        "            # writer.add_histogram(f'before norm msa_{i}', msa, global_step)\n",
        "            msa = nn.functional.layer_norm(msa, [msa.shape[-1]])  # Normalize each channel feature individually\n",
        "            msa = self.dropout(msa) #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "            # writer.add_histogram(f'after_norm msa_{i}', msa, global_step)\n",
        "            x = msa + x # output of MSA 's shape = [32, 768, 196]\n",
        "            # writer.add_histogram(f'after_msa_+_x_{i}', x, global_step)\n",
        "            x = x.permute(0, 2, 1) # shape = [32, 196, 768]\n",
        "            x = nn.functional.layer_norm(x, [x.shape[-1]])  # Normalize each channel feature individually\n",
        "            # writer.add_histogram(f'after_layer_norm_msa_+_x_{i}', x, global_step)\n",
        "            x = torch.cat([x_cls, x], dim=1) # shape = [32, 197, 768]\n",
        "\n",
        "        image_features = self.image_features_proj(x) # shape = [32, 197, 512]\n",
        "        image_features_cls = image_features[:,0,:] # shape = [32, 512]\n",
        "        image_features_cls = image_features_cls.unsqueeze(1) # shape = [32, 1, 512]\n",
        "        image_features_cls = self.dropout(image_features_cls) #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "        image_features = image_features[:,1:,:] # shape = [32, 196, 512]\n",
        "\n",
        "        promptlearner_outputs = self.prompt_learner(image_features, global_step) # shape = [2, 77, 512]\n",
        "        text_features = self.text_encoder(promptlearner_outputs, self.tokenized_prompts) # shape = [2, 512]\n",
        "\n",
        "\n",
        "        # Expand text_features to match image_features' batch size\n",
        "        text_features = text_features.unsqueeze(0) # shape = [1, 2, 512]\n",
        "        text_features_expanded = text_features.expand(image_features.size(0), -1, -1)  # Shape: (32, 2, 512)\n",
        "\n",
        "        # Image patches mean\n",
        "        image_features = image_features.mean(dim = 1) # shape = [32, 512]\n",
        "        image_features = image_features.unsqueeze(1) # shape = [32, 1, 512]\n",
        "\n",
        "        # Normalization\n",
        "        image_features_cls = nn.functional.normalize(image_features_cls, dim=-1)\n",
        "        image_features = nn.functional.normalize(image_features, dim=-1)\n",
        "        text_features_expanded = nn.functional.normalize(text_features_expanded, dim=-1)\n",
        "\n",
        "\n",
        "        image_features_cls = image_features_cls.to(device)  # Shape: [32, 1, 512]\n",
        "        text_features_expanded = text_features_expanded.to(device)  # Shape: [32, 2, 512]\n",
        "        combined_features = torch.cat((image_features_cls, text_features_expanded), dim=1)  # Shape: [32, 3, 512]\n",
        "        # Transpose to match PyTorch's expected shape: [sequence_length, batch_size, embed_dim]\n",
        "        combined_features_t = combined_features.transpose(0, 1).to(device)  # Shape: [3, 32, 512]\n",
        "        # Compute attention\n",
        "        attn_output, attn_weights = self.multihead_attn(\n",
        "            query=combined_features_t,\n",
        "            key=combined_features_t,\n",
        "            value=combined_features_t\n",
        "        )\n",
        "        # Transpose back to [batch_size, sequence_length, embed_dim]\n",
        "        attn_output = attn_output.transpose(0, 1)  # Shape: [32, 3, 512]\n",
        "        updated_image_features_cls = attn_output[:, 0, :]        # Shape: [32, 512]\n",
        "        updated_image_features_cls = updated_image_features_cls.unsqueeze(1)  # Shape: [32, 1, 512]\n",
        "        updated_text_features_expanded = attn_output[:, 1:, :]   # Shape: [32, 2, 512]\n",
        "\n",
        "\n",
        "\n",
        "        #logits = torch.matmul(image_features_cls, text_features_expanded.transpose(1, 2))\n",
        "        #logits = torch.matmul(image_features, text_features_expanded.transpose(1, 2))\n",
        "        logits = torch.matmul(updated_image_features_cls, updated_text_features_expanded.transpose(1, 2))\n",
        "        logits = logits.squeeze(1) # Shape: (32, 2)\n",
        "\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        return logits * logit_scale\n"
      ],
      "metadata": {
        "id": "EIsmbLZYvu2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "knVsEgarCz-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/Zero_Shot_DeepFake_Image_Classification/CLIP_based_deepfake_detection/dataset/CELEB'\n",
        "\n",
        "# Define image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224), interpolation= InterpolationMode.BICUBIC),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]),\n",
        "])\n",
        "\n",
        "# Load the dataset\n",
        "train_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'train'), transform=transform)\n",
        "train_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "val_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'val'), transform=transform)\n",
        "val_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers= os.cpu_count())\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers= os.cpu_count())"
      ],
      "metadata": {
        "id": "mathQ86cC1in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
        "    # Lambda function for warmup phase\n",
        "    def lr_lambda(current_step: int):\n",
        "      if current_step < num_warmup_steps:\n",
        "          return float(current_step) / float(max(1, num_warmup_steps))\n",
        "      return max(\n",
        "          0.0, 0.5 * (1.0 + torch.cos(torch.tensor(torch.pi * (current_step - num_warmup_steps) / (num_training_steps - num_warmup_steps))))\n",
        "      )\n",
        "    return LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, device, num_epochs=10, lr=1e-4, warmup_steps=100, log_interval=100):\n",
        "    model.train()\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    # optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay= 1e-5)\n",
        "    # optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "    optimizer = optim.SGD(\n",
        "                    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                    lr=lr,\n",
        "                    momentum=0.9,\n",
        "                    weight_decay = 1e-4,\n",
        "                    nesterov = True\n",
        "                    )\n",
        "    total_steps = len(train_loader) * num_epochs\n",
        "    scheduler = get_scheduler(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    #criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\")):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            #labels_one_hot = F.one_hot(labels, num_classes=2).float()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():\n",
        "                logits = model(images, global_step=epoch * len(train_loader) + batch_idx)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            # Apply gradient clipping before optimizer step\n",
        "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            scheduler.step()  # Update learning rate after every optimizer step\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            # Log loss to TensorBoard every few batches\n",
        "            # global_step = epoch * len(train_loader) + batch_idx\n",
        "            # writer.add_scalar('Training Loss', loss.item(), global_step)\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Log intermediate loss and accuracy\n",
        "            if (batch_idx + 1) % log_interval == 0:\n",
        "                current_loss = running_loss / total\n",
        "                current_acc = correct / total\n",
        "                print(f\"  Step [{batch_idx + 1}/{len(train_loader)}], \"\n",
        "                      f\"Loss: {current_loss:.4f}, Accuracy: {current_acc:.4f}\")\n",
        "\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = correct / total\n",
        "\n",
        "        val_loss, val_acc = validate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
        "              f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_acc:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "def validate_model(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            #labels_one_hot = F.one_hot(labels, num_classes=2).float()\n",
        "\n",
        "            with autocast():  # Use autocast in validation as well, if mixed precision\n",
        "                logits = model(images)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_loss = running_loss / total\n",
        "    val_acc = correct / total\n",
        "    return val_loss, val_acc"
      ],
      "metadata": {
        "id": "WNVqMhQ7DTiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_train_loader = torch.utils.data.DataLoader(\n",
        "    torch.utils.data.Subset(train_dataset, range(32*10)),\n",
        "    batch_size=32, shuffle=True\n",
        ")\n",
        "small_val_loader = torch.utils.data.DataLoader(\n",
        "    torch.utils.data.Subset(val_dataset, range(32*10)),\n",
        "    batch_size=32, shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "RzL4xWEmv4g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "clip_model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
        "clip_model = clip_model.float()\n",
        "\n",
        "n_ctx = 15  # Number of context tokens\n",
        "classnames = [\"real\", \"fake\"]\n",
        "\n",
        "model = CustomCLIP(n_ctx=n_ctx, classnames=classnames, clip_model=clip_model).to(device)\n",
        "model = torch.nn.DataParallel(model)\n",
        "train_model(model, train_loader, val_loader, device, num_epochs=10, lr=1e-2, warmup_steps=400, log_interval = 20) #lr = 1e-3, warmup_steps = 250\n",
        "#train_model(model, small_train_loader, small_val_loader, device, num_epochs=5, lr=1e-2, warmup_steps=125)\n",
        "# writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "HiMI_PTwDXJs",
        "outputId": "12c42b43-2804-44fc-b27b-7c2d631e051c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-25eec0ba5061>:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "Training Epoch 1/10:   0%|          | 0/500 [00:00<?, ?it/s]<ipython-input-13-25eec0ba5061>:41: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:674.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "Training Epoch 1/10:   4%|▍         | 20/500 [01:26<18:00,  2.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Step [20/500], Loss: 0.6933, Accuracy: 0.5031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/10:   8%|▊         | 40/500 [01:35<03:32,  2.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Step [40/500], Loss: 0.6933, Accuracy: 0.5008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/10:  12%|█▏        | 60/500 [01:44<03:22,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Step [60/500], Loss: 0.6934, Accuracy: 0.5036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/10:  16%|█▌        | 80/500 [01:54<03:12,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Step [80/500], Loss: 0.6933, Accuracy: 0.4980\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/10:  20%|██        | 100/500 [02:03<03:01,  2.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Step [100/500], Loss: 0.6933, Accuracy: 0.4997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/10:  24%|██▍       | 120/500 [02:12<02:53,  2.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Step [120/500], Loss: 0.6934, Accuracy: 0.5018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/10:  25%|██▍       | 124/500 [02:14<06:48,  1.09s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-5b409fd74fa9>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomCLIP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_ctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclip_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#lr = 1e-3, warmup_steps = 250\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#train_model(model, small_train_loader, small_val_loader, device, num_epochs=5, lr=1e-2, warmup_steps=125)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# writer.close()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-25eec0ba5061>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, device, num_epochs, lr, warmup_steps, log_interval)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update learning rate after every optimizer step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;31m# Log loss to TensorBoard every few batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# global_step = epoch * len(train_loader) + batch_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %load_ext tensorboard"
      ],
      "metadata": {
        "id": "LOB4lQfHXy9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "SKPCUKOCXn8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "num_gpus = torch.cuda.device_count()\n",
        "print(f\"Number of available GPUs: {num_gpus}\")"
      ],
      "metadata": {
        "id": "XbgmlsZphJMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing**"
      ],
      "metadata": {
        "id": "MoftDFmxD2-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the testing function\n",
        "def test_model(model, test_loader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    criterion = nn.CrossEntropyLoss()  # Loss function for testing\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient tracking\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            with autocast():  # Use autocast in validation as well, if mixed precision\n",
        "                logits = model(images)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    test_loss = running_loss / total\n",
        "    test_acc = correct / total\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "QDI_fs0DED8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CELEB-Test**"
      ],
      "metadata": {
        "id": "JJ6bA_8MI_rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'test'), transform=transform)\n",
        "test_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers= os.cpu_count())"
      ],
      "metadata": {
        "id": "yvFF-RbCD4lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model after training\n",
        "test_model(model, test_loader, device)"
      ],
      "metadata": {
        "id": "0wQHUwC-EGtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CELEB-M**"
      ],
      "metadata": {
        "id": "HaeP1i46YE1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = '/content/drive/MyDrive/Zero_Shot_DeepFake_Image_Classification/CLIP_based_deepfake_detection/dataset/CELEB-M'\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(dir, 'test'), transform=transform)\n",
        "test_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers= os.cpu_count())"
      ],
      "metadata": {
        "id": "eMWVB_MaYE1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model after training\n",
        "test_model(model, test_loader, device)"
      ],
      "metadata": {
        "id": "WZaXuZCTYE1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FS**"
      ],
      "metadata": {
        "id": "B-LOBIofYLAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = '/content/drive/MyDrive/Zero_Shot_DeepFake_Image_Classification/CLIP_based_deepfake_detection/dataset/FS'\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(dir, 'test'), transform=transform)\n",
        "test_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers= os.cpu_count())"
      ],
      "metadata": {
        "id": "ywTod3gaYLAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model after training\n",
        "test_model(model, test_loader, device)"
      ],
      "metadata": {
        "id": "GuseiNQCYLAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NT**"
      ],
      "metadata": {
        "id": "C4OI2qNcJ4H7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = '/content/drive/MyDrive/Zero_Shot_DeepFake_Image_Classification/CLIP_based_deepfake_detection/dataset/NT'\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(dir, 'test'), transform=transform)\n",
        "test_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers= os.cpu_count())"
      ],
      "metadata": {
        "id": "ez2YL8U9J4H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model after training\n",
        "test_model(model, test_loader, device)"
      ],
      "metadata": {
        "id": "_zp1t3YsJ4H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DF**"
      ],
      "metadata": {
        "id": "2YAWPQs4KPtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = '/content/drive/MyDrive/Zero_Shot_DeepFake_Image_Classification/CLIP_based_deepfake_detection/dataset/DF'\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(dir, 'test'), transform=transform)\n",
        "test_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers= os.cpu_count())"
      ],
      "metadata": {
        "id": "1nbWt6nIKPtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model after training\n",
        "test_model(model, test_loader, device)"
      ],
      "metadata": {
        "id": "S62Djg4cKPtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DFD**"
      ],
      "metadata": {
        "id": "MHEX3FiHLUw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = '/content/drive/MyDrive/Zero_Shot_DeepFake_Image_Classification/CLIP_based_deepfake_detection/dataset/DFD'\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(dir, 'test'), transform=transform)\n",
        "test_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers= os.cpu_count())"
      ],
      "metadata": {
        "id": "UEaT19PTLUw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model after training\n",
        "test_model(model, test_loader, device)"
      ],
      "metadata": {
        "id": "Ymn2NaBkLUw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **F2F**"
      ],
      "metadata": {
        "id": "-UDHXxUbLbV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = '/content/drive/MyDrive/Zero_Shot_DeepFake_Image_Classification/CLIP_based_deepfake_detection/dataset/F2F'\n",
        "test_dataset = datasets.ImageFolder(root=os.path.join(dir, 'test'), transform=transform)\n",
        "test_dataset.class_to_idx = {'real': 0, 'fake': 1}\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers= os.cpu_count())"
      ],
      "metadata": {
        "id": "cJN0h9azLbV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model after training\n",
        "test_model(model, test_loader, device)"
      ],
      "metadata": {
        "id": "Ylv15-IPLbV4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}